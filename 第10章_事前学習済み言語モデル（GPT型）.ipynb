{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "ADPET68xjlzr",
        "tags": []
      },
      "source": [
        "# 第10章: 事前学習済み言語モデル（GPT型）\n",
        "\n",
        "本章では、GPT型（Transformerのデコーダ型）の事前学習済みモデルを利用して、言語生成、評判分析器（ポジネガ分類器）の構築、ファインチューニング、強化学習などに取り組む。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "C1xKmMckti92",
        "tags": []
      },
      "source": [
        "## 90. 次単語予測\n",
        "\n",
        "“The movie was full of\"に続くトークン（トークン列ではなく一つのトークンであることに注意せよ）として適切なもの上位10個と、その確率（尤度）を求めよ。ただし、言語モデルへのプロンプトがどのようなトークン列に変換されたか、確認せよ。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:\n",
            "  1. [ID]: 464, [Token]: 'The'\n",
            "  2. [ID]: 3807, [Token]: ' movie'\n",
            "  3. [ID]: 373, [Token]: ' was'\n",
            "  4. [ID]: 1336, [Token]: ' full'\n",
            "  5. [ID]: 286, [Token]: ' of'\n",
            "\n",
            "Top 10 next tokens predictions:\n",
            "  1. [Token]: ' jokes', [Probability]: 0.021892\n",
            "  2. [Token]: ' great', [Probability]: 0.018644\n",
            "  3. [Token]: ' laughs', [Probability]: 0.011524\n",
            "  4. [Token]: ' bad', [Probability]: 0.010874\n",
            "  5. [Token]: ' surprises', [Probability]: 0.010667\n",
            "  6. [Token]: ' references', [Probability]: 0.010528\n",
            "  7. [Token]: ' fun', [Probability]: 0.009992\n",
            "  8. [Token]: ' humor', [Probability]: 0.007415\n",
            "  9. [Token]: ' \"', [Probability]: 0.007408\n",
            "  10. [Token]: ' the', [Probability]: 0.006709\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_name = \"openai-community/gpt2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "sentence = \"The movie was full of\"\n",
        "\n",
        "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "input_ids = inputs.input_ids\n",
        "\n",
        "print(\"Input:\")\n",
        "token_ids = input_ids[0].tolist()\n",
        "tokens = [tokenizer.decode([token_id]) for token_id in token_ids]\n",
        "for i, (token_id, token) in enumerate(zip(token_ids, tokens)):\n",
        "    print(f\"  {i+1}. [ID]: {token_id}, [Token]: '{token}'\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits[:, -1, :]  \n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=-1)[0]\n",
        "\n",
        "top_k = 10\n",
        "top_k_probs, top_k_indices = torch.topk(probabilities, top_k)\n",
        "\n",
        "print(f\"\\nTop {top_k} next tokens predictions:\")\n",
        "for i, (prob, idx) in enumerate(zip(top_k_probs, top_k_indices)):\n",
        "    token = tokenizer.decode([idx])\n",
        "    print(f\"  {i+1}. [Token]: '{token}', [Probability]: {prob:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "s1RhOldA0meh",
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "## 91. 続きのテキストの予測\n",
        "\n",
        "“The movie was full of\"に続くテキストを複数予測せよ。このとき、デコーディングの方法や温度パラメータ（temperature）を変えながら、予測される複数のテキストの変化を観察せよ。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "text with temperature 0.5:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  1. 'The movie was full of great characters and great action. It was a big hit, and I was really happy to see it. It's a very unique movie and it's very unique in that the movie has a lot of great characters and great action'\n",
            "\n",
            "text with temperature 1.0:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  1. 'The movie was full of scenes of the people of the city that were destroyed or just a lot of empty places that weren't anything special,\" says Ben Johnson, who worked on the film as a security engineer. \"Things that you just don't see'\n",
            "\n",
            "text with temperature 1.5:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  1. 'The movie was full of plot and a fair bit of comedy that also reminded me more and greater of my old \"Kurt-a-loo movie\" films about dinosaurs in my youth... but a bit bland, didn't keep you interested or'\n",
            "\n",
            "text with temperature 2.0:\n",
            "  1. 'The movie was full of nudity and graphic sexual themes, a huge increase upon a 2012 debut starring Tatum. Fans often accused The Good Witch being violent and sexualized on that movie with plenty of hot action as they waited until 2026 to do \"'\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"openai-community/gpt2\")\n",
        "sentence = \"The movie was full of\"\n",
        "\n",
        "temperature = [0.5, 1.0, 1.5, 2.0]\n",
        "for temp in temperature:\n",
        "    print(f\"\\ntext with temperature {temp}:\")\n",
        "    generated_text = generator(sentence, max_length=50, num_return_sequences=1, temperature=temp)\n",
        "    for i, text in enumerate(generated_text):\n",
        "        print(f\"  {i+1}. '{text['generated_text']}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "7ZFadg6B8VdA",
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "## 92. 予測されたテキストの確率を計算\n",
        "\n",
        "“The movie was full of\"に続くテキストを予測し、生成された各単語の尤度を表示せよ（生成されるテキストが長いと出力が読みにくくなるので、適当な長さで生成を打ち切るとよい）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Generated text]:\n",
            "The movie was full of jokes and jokes about how the movie was a joke. It was a joke about how the movie was\n",
            " 1. [Token]: ' jokes', [Probability]: 0.021892\n",
            " 2. [Token]: ' and', [Probability]: 0.289225\n",
            " 3. [Token]: ' jokes', [Probability]: 0.098501\n",
            " 4. [Token]: ' about', [Probability]: 0.205558\n",
            " 5. [Token]: ' how', [Probability]: 0.099715\n",
            " 6. [Token]: ' the', [Probability]: 0.084637\n",
            " 7. [Token]: ' movie', [Probability]: 0.036412\n",
            " 8. [Token]: ' was', [Probability]: 0.296344\n",
            " 9. [Token]: ' a', [Probability]: 0.067677\n",
            "10. [Token]: ' joke', [Probability]: 0.173507\n",
            "11. [Token]: '.', [Probability]: 0.280386\n",
            "12. [Token]: ' It', [Probability]: 0.123000\n",
            "13. [Token]: ' was', [Probability]: 0.519725\n",
            "14. [Token]: ' a', [Probability]: 0.149313\n",
            "15. [Token]: ' joke', [Probability]: 0.268987\n",
            "16. [Token]: ' about', [Probability]: 0.424155\n",
            "17. [Token]: ' how', [Probability]: 0.174168\n",
            "18. [Token]: ' the', [Probability]: 0.123645\n",
            "19. [Token]: ' movie', [Probability]: 0.616076\n",
            "20. [Token]: ' was', [Probability]: 0.634964\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "model_name = \"openai-community/gpt2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "sentence = \"The movie was full of\"\n",
        "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "input_ids = inputs.input_ids\n",
        "\n",
        "\n",
        "max_new_tokens = 20\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True\n",
        "    )\n",
        "\n",
        "generated_ids = outputs.sequences[0]\n",
        "generated_tokens = generated_ids.tolist()\n",
        "new_tokens = generated_tokens[input_ids.shape[1]:]  \n",
        "\n",
        "log_probs = []\n",
        "for i, score in enumerate(outputs.scores):\n",
        "    probs = F.softmax(score[0], dim=-1)\n",
        "    token_id = new_tokens[i]\n",
        "    prob = probs[token_id].item()\n",
        "    log_probs.append(prob)\n",
        "\n",
        "\n",
        "print(\"\\n[Generated text]:\")\n",
        "print(tokenizer.decode(generated_ids))\n",
        "\n",
        "for i, (token_id, prob) in enumerate(zip(new_tokens, log_probs), start=1):\n",
        "    token_str = tokenizer.decode([token_id])\n",
        "    print(f\"{i:2d}. [Token]: '{token_str}', [Probability]: {prob:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "FvNCTMj6OegF",
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "## 93. パープレキシティ\n",
        "\n",
        "適当な文を準備して、事前学習済み言語モデルでパープレキシティを測定せよ。例えば、\n",
        "\n",
        "+ The movie was full of surprises\n",
        "+ The movies were full of surprises\n",
        "+ The movie were full of surprises\n",
        "+ The movies was full of surprises\n",
        "\n",
        "の4文に対して、パープレキシティを測定して観察せよ（最後の2つの文は故意に文法的な間違いを入れた）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'The movie was full of surprises' → Perplexity: 99.35\n",
            "'The movies were full of surprises' → Perplexity: 126.48\n",
            "'The movie were full of surprises' → Perplexity: 278.88\n",
            "'The movies was full of surprises' → Perplexity: 274.66\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import math\n",
        "\n",
        "model_name = \"openai-community/gpt2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model.eval()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    \n",
        "sentences = [\n",
        "    \"The movie was full of surprises\",\n",
        "    \"The movies were full of surprises\",\n",
        "    \"The movie were full of surprises\",\n",
        "    \"The movies was full of surprises\"\n",
        "]\n",
        "\n",
        "def calculate_perplexity(sentence):\n",
        "    encodings = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    input_ids = encodings.input_ids\n",
        "    if torch.cuda.is_available():\n",
        "        input_ids = input_ids.cuda()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "        perplexity = torch.exp(loss).item()\n",
        "    return perplexity\n",
        "\n",
        "for sentence in sentences:\n",
        "    ppl = calculate_perplexity(sentence)\n",
        "    print(f\"'{sentence}' → Perplexity: {ppl:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-7fB-n9suYg"
      },
      "source": [
        "## 94. チャットテンプレート\n",
        "\n",
        "\"What do you call a sweet eaten after dinner?\"という問いかけに対する応答を生成するため、チャットテンプレートを適用し、言語モデルに与えるべきプロンプトを作成せよ。また、そのプロンプトに対する応答を生成し、表示せよ。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 25 Jun 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "What do you call a sweet eaten after dinner?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Response: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 25 Jun 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "What do you call a sweet eaten after dinner?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Some common names for a sweet eaten after dinner include:\n",
            "\n",
            "1. Dessert\n",
            "2. Sweet\n",
            "3. Snack\n",
            "4. Dessert\n",
            "5. Sweet treat\n",
            "6. Candy\n",
            "7. Dessert\n",
            "8. Sweetie\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, pipeline\n",
        "import os\n",
        "\n",
        "access_token = os.environ[\"HUGGING_FACE_TOKEN\"]\n",
        "\n",
        "generator = pipeline(\"text-generation\", model = \"meta-llama/Llama-3.2-1B-Instruct\", token=access_token)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", token=access_token)\n",
        "\n",
        "chat = [\n",
        "    {\"role\": \"user\", \"content\": \"What do you call a sweet eaten after dinner?\"}\n",
        "]\n",
        "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "print(f\"Prompt: {prompt}\")\n",
        "response = generator(prompt, max_new_tokens=50, do_sample=True, temperature=0.7)\n",
        "print(f\"Response: {response[0]['generated_text']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "PT-bk0XWIZ2E",
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "## 95. マルチターンのチャット\n",
        "\n",
        "問題94で生成された応答に対して、追加で\"Please give me the plural form of the word with its spelling in reverse order.\"と問いかけたときの応答を生成・表示せよ。また、その時に言語モデルに与えるプロンプトを確認せよ。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 25 Jun 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "What do you call a sweet eaten after dinner?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Some common names for a sweet eaten after dinner include:\n",
            "\n",
            "1. Dessert\n",
            "2. Sweet\n",
            "3. Snack\n",
            "4. Dessert\n",
            "5. Sweet treat\n",
            "6. Candy\n",
            "7. Dessert\n",
            "8. Sweetie<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Please give me the plural form of the word with its spelling in reverse order.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 25 Jun 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "What do you call a sweet eaten after dinner?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Some common names for a sweet eaten after dinner include:\n",
            "\n",
            "1. Dessert\n",
            "2. Sweet\n",
            "3. Snack\n",
            "4. Dessert\n",
            "5. Sweet treat\n",
            "6. Candy\n",
            "7. Dessert\n",
            "8. Sweetie<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Please give me the plural form of the word with its spelling in reverse order.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "The plural form of the word 'dessert' with its spelling in reverse order is 'tsersed'.\n",
            "\n",
            "Extracted new response:\n",
            "The plural form of the word 'dessert' with its spelling in reverse order is 'tsersed'.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, pipeline\n",
        "import os\n",
        "\n",
        "access_token = os.environ[\"HUGGING_FACE_TOKEN\"]\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-1B-Instruct\", token=access_token)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", token=access_token)\n",
        "\n",
        "assistant_response = response[0]['generated_text'].replace(prompt, \"\")\n",
        "\n",
        "chat = [\n",
        "    {\"role\": \"user\", \"content\": \"What do you call a sweet eaten after dinner?\"},\n",
        "    {\"role\": \"assistant\", \"content\": assistant_response},\n",
        "    {\"role\": \"user\", \"content\": \"Please give me the plural form of the word with its spelling in reverse order.\"}\n",
        "]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "print(prompt)\n",
        "\n",
        "response = generator(prompt, max_new_tokens=100, do_sample=True, temperature=0.7)\n",
        "print(response[0]['generated_text'])\n",
        "\n",
        "new_response = response[0]['generated_text'].replace(prompt, \"\")\n",
        "print(\"\\nExtracted new response:\")\n",
        "print(new_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "qH0YortL0afd",
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "## 96. プロンプトによる感情分析\n",
        "\n",
        "事前学習済み言語モデルで感情分析を行いたい。テキストを含むプロンプトを事前学習済み言語モデルに与え、（ファインチューニングは行わずに）テキストのポジネガを予測するという戦略で、[SST-2](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip)の開発データにおける正解率を測定せよ。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on SST-2 dev set: 0.5791\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, pipeline\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "\n",
        "access_token = os.environ[\"HUGGING_FACE_TOKEN\"]\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", token=access_token)\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    tokenizer=tokenizer,\n",
        "    token=access_token,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "dev_dataset = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n",
        "\n",
        "def sentiment_analysis(sentence):\n",
        "    prompt = (\n",
        "        \"You are a professional sentiment analysis model. Your task is to classify the sentiment of the given sentence as positive or negative. If the sentiment is positive, respond with '1'. If it is negative, respond with '0'.\\n\\n\"\n",
        "        f\"Sentence: {sentence}\\nSentiment:\"\n",
        "    )\n",
        "    response = generator(prompt, max_new_tokens=5, return_full_text=False)[0][\"generated_text\"]\n",
        "    prediction = response.strip()[0]\n",
        "    return prediction\n",
        "\n",
        "\n",
        "correct = 0\n",
        "total = len(dev_dataset)\n",
        "\n",
        "for example in dev_dataset:\n",
        "    sentence = example[\"sentence\"]\n",
        "    label = str(example[\"label\"]) \n",
        "    pred = sentiment_analysis(sentence)\n",
        "    if pred == label:\n",
        "        correct += 1\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Accuracy on SST-2 dev set: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "giA6FivrKaSf",
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "## 97. 埋め込みに基づく感情分析\n",
        "\n",
        "事前学習済み言語モデルでテキストをベクトルで表現（エンコード）し、そのベクトルにフィードフォワード層を通すことで極性ラベルを予測するモデルを学習せよ。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25886c1f8eba48d8bcf17c41f6c4bea8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6408814a91b04ee483e5955b56fbce29",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c30ba00225f468bae9ecab21c3538c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cef5608faed24ca7a3a22d2526599555",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "622b79800ac747b98e28d934fbdb95af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20f41669792d4eefb889bb44fc338252",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27a04c0f0a2a480888bd9fa14162439c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 1/3:   0%|          | 0/1053 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a25366500aef4d4c9bad28817506156c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/14 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss = 0.6801, Validation Accuracy = 0.7764\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff14a3cd74754f569473090c1947f0ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 2/3:   0%|          | 0/1053 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90014e39d253464f9f0cf0e74402d7f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/14 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Train Loss = 0.5236, Validation Accuracy = 0.7947\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c48a46ebac94fce8c14fb6c97bc70cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 3/3:   0%|          | 0/1053 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bfbee8229984418397847503c94bbb73",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/14 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Train Loss = 0.4673, Validation Accuracy = 0.8016\n",
            "Best validation accuracy: 0.8016\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "train_dataset = load_dataset(\"glue\", \"sst2\", split=\"train\")\n",
        "dev_dataset = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "encoder = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "def get_embeddings(texts, batch_size=32):\n",
        "    all_embeddings = []\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "        input_ids = inputs[\"input_ids\"]\n",
        "        attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            last_hidden = outputs.last_hidden_state  \n",
        "\n",
        "            lengths = attention_mask.sum(dim=1) - 1  \n",
        "            batch_indices = torch.arange(input_ids.size(0))\n",
        "            embeddings = last_hidden[batch_indices, lengths] \n",
        "            embeddings = embeddings.cpu().numpy()\n",
        "\n",
        "        all_embeddings.append(embeddings)\n",
        "\n",
        "    return np.vstack(all_embeddings)\n",
        "\n",
        "train_texts = train_dataset[\"sentence\"]\n",
        "train_labels = train_dataset[\"label\"]\n",
        "train_embeddings = get_embeddings(train_texts)\n",
        "\n",
        "dev_texts = dev_dataset[\"sentence\"]\n",
        "dev_labels = dev_dataset[\"label\"]\n",
        "dev_embeddings = get_embeddings(dev_texts)\n",
        "\n",
        "train_data = TensorDataset(\n",
        "    torch.tensor(train_embeddings, dtype=torch.float32),\n",
        "    torch.tensor(train_labels, dtype=torch.long)\n",
        ")\n",
        "dev_data = TensorDataset(\n",
        "    torch.tensor(dev_embeddings, dtype=torch.float32),\n",
        "    torch.tensor(dev_labels, dtype=torch.long)\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "dev_loader = DataLoader(dev_data, batch_size=64)\n",
        "\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=768, hidden_dim=256):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(x)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "classifier = SentimentClassifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=3e-5)\n",
        "\n",
        "num_epochs = 3\n",
        "best_accuracy = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    classifier.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for embeddings, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        embeddings, labels = embeddings.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = classifier(embeddings)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "    classifier.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for embeddings, labels in tqdm(dev_loader, desc=\"Evaluating\"):\n",
        "            embeddings, labels = embeddings.to(device), labels.to(device)\n",
        "            outputs = classifier(embeddings)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Validation Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        torch.save(classifier.state_dict(), \"sentiment_classifier.pt\")\n",
        "\n",
        "print(f\"Best validation accuracy: {best_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "UnREZD3nTWUr",
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "## 98. ファインチューニング\n",
        "\n",
        "問題96のプロンプトに対して、正解の感情ラベルをテキストの応答として返すように事前学習済みモデルをファインチューニングせよ。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79d4e7a727d44c0ca711e35db1b9fda0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 3,407,872 || all params: 1,239,222,272 || trainable%: 0.2750\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kawata/.local/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "/home/kawata/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/home/kawata/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25254' max='25254' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25254/25254 8:55:35, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.596100</td>\n",
              "      <td>1.226195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.450100</td>\n",
              "      <td>1.327517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.383700</td>\n",
              "      <td>1.419619</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kawata/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/home/kawata/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/home/kawata/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/home/kawata/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/home/kawata/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig \n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig, \n",
        "    get_peft_model, \n",
        "    prepare_model_for_kbit_training\n",
        ") \n",
        "\n",
        "dataset = load_dataset(\"glue\", \"sst2\")\n",
        "train_data = dataset[\"train\"]\n",
        "val_data = dataset[\"validation\"]\n",
        "\n",
        "def format_prompt(example):\n",
        "    return {\n",
        "        \"text\": (\n",
        "            \"You are a professional sentiment analysis model. \"\n",
        "            \"Your task is to classify the sentiment of the given sentence as positive or negative. \"\n",
        "            \"If it is positive, respond with '1'. If it is negative, respond with '0'.\\n\\n\"\n",
        "            f\"Sentence: {example['sentence']}\\nSentiment: {example['label']}\"\n",
        "        )\n",
        "    }\n",
        "\n",
        "train_data = train_data.map(format_prompt)\n",
        "val_data = val_data.map(format_prompt)\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "access_token = os.environ.get(\"HUGGING_FACE_TOKEN\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize(example):\n",
        "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
        "\n",
        "train_tokenized = train_data.map(tokenize, batched=True, remove_columns=train_data.column_names)\n",
        "val_tokenized = val_data.map(tokenize, batched=True, remove_columns=val_data.column_names)\n",
        "\n",
        "\n",
        "# LoRA + 8ビット量子化\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    token=access_token,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0} \n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRAの設定\n",
        "lora_config = LoraConfig(\n",
        "    r=16, # LoRAアダプターのランク\n",
        "    lora_alpha=32, # スケーリング用のアルファ値\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], # LoRAを適用する層\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# ベースモデルをLoRAでラップ\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./model/finetuned-llama-sst2-lora\",\n",
        "    per_device_train_batch_size=2,   \n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,   \n",
        "    learning_rate=2e-4,              \n",
        "    evaluation_strategy=\"epoch\",     \n",
        "    save_strategy=\"epoch\",           \n",
        "    num_train_epochs=3,              \n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50,                \n",
        "    fp16=torch.cuda.is_available(), \n",
        "    report_to=[\"tensorboard\"],       \n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=val_tokenized,\n",
        "    data_collator=data_collator,   \n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(\"./model/finetuned-llama-sst2-lora-final\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.6571\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"./model/finetuned-llama-sst2-lora-final\", tokenizer=tokenizer)\n",
        "\n",
        "def predict(sentence):\n",
        "    prompt = (\n",
        "        \"You are a professional sentiment analysis model. \"\n",
        "        \"Your task is to classify the sentiment of the given sentence as positive or negative. \"\n",
        "        \"If it is positive, respond with '1'. If it is negative, respond with '0'.\\n\\n\"\n",
        "        f\"Sentence: {sentence}\\nSentiment:\"\n",
        "    )\n",
        "    response = pipe(prompt, max_new_tokens=5, return_full_text=False)[0]['generated_text'].strip()\n",
        "    return response[0]\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for ex in val_data:\n",
        "    pred = predict(ex[\"sentence\"])\n",
        "    gold = str(ex[\"label\"])\n",
        "    if pred == gold:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "\n",
        "print(f\"Validation Accuracy: {correct / total:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "4f0St5Ce0l34",
        "tags": []
      },
      "source": [
        "## 99. 選好チューニング\n",
        "\n",
        "問題96のプロンプトに対して、正解の感情ラベルを含むテキストを望ましい応答、間違った感情ラベルを含むテキストを望ましくない応答として、事前学習済み言語モデルを選好チューニング (preference tuning) を実施せよ。選好チューニングのアルゴリズムとしては、近傍方策最適化 (PPO: Proximal Policy Optimization) や直接選好最適化 (DPO: Direct Preference Optimization) などが考えられる。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-27 00:25:25.245756: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-06-27 00:25:25.277266: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-06-27 00:25:25.277298: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-06-27 00:25:25.278348: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-27 00:25:25.283830: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-06-27 00:25:28.512214: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f2c93cda0dd458d9e1e4e86dd06e4b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting prompt in train dataset:   0%|          | 0/67349 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b21beaf08f164a57bfc47d0480ad9b5b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying chat template to train dataset:   0%|          | 0/67349 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fdb47204a0ec4f5db1faa9d7b7c4f7e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/67349 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "/home/kawata/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/home/kawata/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/home/kawata/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/home/kawata/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8420' max='8420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8420/8420 6:53:52, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.266400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.187900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.180900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.173200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.164900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.162600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.164200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.161500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kawata/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/home/kawata/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/home/kawata/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/home/kawata/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, \n",
        "    AutoTokenizer, \n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "\n",
        "access_token = os.environ.get(\"HUGGING_FACE_TOKEN\")\n",
        "dataset = load_dataset(\"glue\", \"sst2\")\n",
        "\n",
        "prompt = (\n",
        "    \"You are a professional sentiment analysis model. \"\n",
        "    \"Your task is to classify the sentiment of the given sentence as positive or negative. \"\n",
        "    \"If the sentiment is positive, respond with '1'. If it is negative, respond with '0'.\\n\\n\"\n",
        ")\n",
        "\n",
        "def create_dpo_data(examples, prompt):\n",
        "    chosen_responses = []\n",
        "    rejected_responses = []\n",
        "    prompts = []\n",
        "    \n",
        "    for sentence, label in zip(examples[\"sentence\"], examples[\"label\"]):\n",
        "        current_prompt = prompt + f\"Sentence: {sentence}\\nSentiment:\"\n",
        "        correct_response = str(label)\n",
        "        incorrect_response = \"0\" if label == 1 else \"1\"\n",
        "        \n",
        "        prompts.append(current_prompt)\n",
        "        chosen_responses.append(correct_response)\n",
        "        rejected_responses.append(incorrect_response)\n",
        "    \n",
        "    return {\n",
        "        \"prompt\": prompts,\n",
        "        \"chosen\": chosen_responses,\n",
        "        \"rejected\": rejected_responses\n",
        "    }\n",
        "\n",
        "train_dpo_dict = create_dpo_data(dataset[\"train\"], prompt)\n",
        "train_dpo_dataset = Dataset.from_dict(train_dpo_dict)\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token)\n",
        "tokenizer.pad_token = tokenizer.eos_token \n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    token=access_token,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "dpo_config = DPOConfig(\n",
        "    output_dir=\"./model/llama-sst2-dpo\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=1e-5,\n",
        "    num_train_epochs=2,\n",
        "    logging_steps=1000,\n",
        "    optim=\"adamw_torch\",\n",
        "    report_to=\"tensorboard\",\n",
        "    save_strategy=\"epoch\",\n",
        "    bf16=torch.cuda.is_available(),\n",
        "    save_total_limit=1,\n",
        "    beta=0.1,\n",
        "    max_length=512,\n",
        "    padding_value=tokenizer.pad_token_id\n",
        ")\n",
        "\n",
        "dpo_trainer = DPOTrainer(\n",
        "    model=model,\n",
        "    ref_model=None, \n",
        "    args=dpo_config,\n",
        "    train_dataset=train_dpo_dataset,\n",
        ")\n",
        "\n",
        "dpo_trainer.train()\n",
        "dpo_trainer.save_model(\"./model/llama-sst2-dpo-final\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db5a2d6a83164d3190dfa8e65548b595",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/872 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy after DPO fine-tuning: 0.9472\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, AutoTokenizer\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "\n",
        "def evaluate_model():\n",
        "    dataset = load_dataset(\"glue\", \"sst2\")\n",
        "    access_token = os.environ.get(\"HUGGING_FACE_TOKEN\")\n",
        "    model_path = \"./model/llama-sst2-dpo-final\"\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path, token=access_token)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    generator = pipeline(\n",
        "        \"text-generation\", \n",
        "        model=model_path,\n",
        "        tokenizer=tokenizer,\n",
        "        token=access_token\n",
        "    )\n",
        "    \n",
        "    correct = 0\n",
        "    total = len(dataset[\"validation\"])\n",
        "    \n",
        "    for example in tqdm(dataset[\"validation\"]):\n",
        "        sentence = example[\"sentence\"]\n",
        "        label = str(example[\"label\"])\n",
        "        \n",
        "        prompt = (\n",
        "            \"You are a professional sentiment analysis model. \"\n",
        "            \"Your task is to classify the sentiment of the given sentence as positive or negative. \"\n",
        "            \"If the sentiment is positive, respond with '1'. If it is negative, respond with '0'.\\n\\n\"\n",
        "            f\"Sentence: {sentence}\\nSentiment:\"\n",
        "        )\n",
        "        \n",
        "        response = generator(prompt, max_new_tokens=5, return_full_text=False)[0][\"generated_text\"]\n",
        "        pred = response.strip()[0] \n",
        "        \n",
        "        if pred == label:\n",
        "            correct += 1\n",
        "    \n",
        "    return correct / total\n",
        "\n",
        "accuracy = evaluate_model()\n",
        "print(f\"Accuracy after DPO fine-tuning: {accuracy:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a56790cc58c4155bb4fb62352fe6853": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c4e978f772946f293451de4484720ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da96a5f6901649958e682be81f939149",
            "placeholder": "​",
            "style": "IPY_MODEL_3f9c4fcaa07b42a0971b99205dd05458",
            "value": " 1/1 [00:00&lt;00:00, 25.67it/s]"
          }
        },
        "0f1172658036407993eb46da17554501": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd75431e52b54d9bb2fd7c06737c28f5",
            "placeholder": "​",
            "style": "IPY_MODEL_12c4ec93b2fc4b239b775d5b947b253f",
            "value": " 67349/0 [00:00&lt;00:00, 674165.72 examples/s]"
          }
        },
        "12c4ec93b2fc4b239b775d5b947b253f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1411c82bf2f3475a826a145fb9f89626": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "145b34f60df1429e8685bcc9b2f05be2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c8a470363f246d692c48ab4a502c4c7",
            "max": 872,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae5b03ae867f4ffba662a8888b319926",
            "value": 872
          }
        },
        "14c91ad60bc84fb0a9c6cc1eb9365425": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b91de93f8ec46c2ac153f40f50ae2e0",
            "placeholder": "​",
            "style": "IPY_MODEL_f4367d9584aa42e0b4249b33af0435d9",
            "value": "Generating train split: "
          }
        },
        "16763c09bfa34f03821316fe7c9902e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c57bd417124c4faa8652b13e826405db",
              "IPY_MODEL_3248b0100e614e2a89b0e2dc13be0ad5",
              "IPY_MODEL_936c040e05bc4b2aadd82245d0c2f3b3"
            ],
            "layout": "IPY_MODEL_a1aac38df0bd43bf9a48a55be029f499"
          }
        },
        "1860eb6e26ca45aeab8ab4ab41334a25": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29245bfd86f0460ea2bfccaca75690a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e33f53fccce4875bbb245afe5b87cf3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5cbbe2769e024a92a5aa718d8f889d0f",
            "value": 1
          }
        },
        "2b91de93f8ec46c2ac153f40f50ae2e0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3248b0100e614e2a89b0e2dc13be0ad5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93d07d6d16da45b2a248ad37e51c3180",
            "max": 67349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a03ab49faf2a4027906a0e9f806bd2db",
            "value": 67349
          }
        },
        "3c41fe7af38f49f581ae359b5d970d23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f95aef5e2c74acd8ffbfad2543b2102",
            "placeholder": "​",
            "style": "IPY_MODEL_9a20391390dd4f96aaea37321b4dfb53",
            "value": " 872/0 [00:00&lt;00:00, 66375.69 examples/s]"
          }
        },
        "3f6a346c2abd42ea9fb5af9441376472": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97484a182f634ec8a711d516fdeb0c63",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7453ed4be064ab586858e8dec61ffae",
            "value": 1
          }
        },
        "3f95aef5e2c74acd8ffbfad2543b2102": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f9c4fcaa07b42a0971b99205dd05458": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b8bf221e7974b65b4dde96219ce0f47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bc6646a088f4bd7b58b33ae1bbab1ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d1eef416f5944c2911154fb770d869f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54587b2141984fd495bf55ea6df0d1a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d603aabb8b6443d2a66b94701858f523",
            "placeholder": "​",
            "style": "IPY_MODEL_1411c82bf2f3475a826a145fb9f89626",
            "value": "Generating dev split: "
          }
        },
        "5cbbe2769e024a92a5aa718d8f889d0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "64ebcabb9c26423f8cda28966d9bbedb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "678df530cc994774899eb213581f737b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c8a470363f246d692c48ab4a502c4c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75a29e01568b4272832f3e00dd92a707": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80837417e00b4fd3814524786898697c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81b067d9308141c786bc70f709681ce7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_54587b2141984fd495bf55ea6df0d1a5",
              "IPY_MODEL_29245bfd86f0460ea2bfccaca75690a1",
              "IPY_MODEL_3c41fe7af38f49f581ae359b5d970d23"
            ],
            "layout": "IPY_MODEL_4bc6646a088f4bd7b58b33ae1bbab1ae"
          }
        },
        "8dec4107d6a44cdaae9020345e2c25a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e33f53fccce4875bbb245afe5b87cf3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "936c040e05bc4b2aadd82245d0c2f3b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64ebcabb9c26423f8cda28966d9bbedb",
            "placeholder": "​",
            "style": "IPY_MODEL_678df530cc994774899eb213581f737b",
            "value": " 67349/67349 [00:03&lt;00:00, 20324.02 examples/s]"
          }
        },
        "93d07d6d16da45b2a248ad37e51c3180": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97484a182f634ec8a711d516fdeb0c63": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "9a20391390dd4f96aaea37321b4dfb53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a03ab49faf2a4027906a0e9f806bd2db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a1aac38df0bd43bf9a48a55be029f499": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3164d72e2d5431dbd7921b74290193a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3cd48beab0741b99a7885f89ebe1181": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8eb7b58bb3a4bfc9d4dad2bc0857dbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4da859c892b42cfa6c909e7dfe2df7d",
              "IPY_MODEL_df8979acad044b40bb55c3e00c418e05",
              "IPY_MODEL_0c4e978f772946f293451de4484720ed"
            ],
            "layout": "IPY_MODEL_b913e33aebe642cca6fa8a6ade682988"
          }
        },
        "ae5b03ae867f4ffba662a8888b319926": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b1763f6b7fc9430ab2f773279fdd954d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75a29e01568b4272832f3e00dd92a707",
            "placeholder": "​",
            "style": "IPY_MODEL_8dec4107d6a44cdaae9020345e2c25a4",
            "value": "Map: 100%"
          }
        },
        "b913e33aebe642cca6fa8a6ade682988": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd75431e52b54d9bb2fd7c06737c28f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0929c9ec77a48df8a7fd3e2f4539a51": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c57bd417124c4faa8652b13e826405db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1860eb6e26ca45aeab8ab4ab41334a25",
            "placeholder": "​",
            "style": "IPY_MODEL_4b8bf221e7974b65b4dde96219ce0f47",
            "value": "Map: 100%"
          }
        },
        "d5e5c6c318cc41c4ac40647e6d5a5106": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14c91ad60bc84fb0a9c6cc1eb9365425",
              "IPY_MODEL_3f6a346c2abd42ea9fb5af9441376472",
              "IPY_MODEL_0f1172658036407993eb46da17554501"
            ],
            "layout": "IPY_MODEL_0a56790cc58c4155bb4fb62352fe6853"
          }
        },
        "d603aabb8b6443d2a66b94701858f523": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7453ed4be064ab586858e8dec61ffae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da96a5f6901649958e682be81f939149": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "deee5780a392426096c2ff4f76aa1547": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df8979acad044b40bb55c3e00c418e05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec97a156272345ba8d456c5f090d9412",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_deee5780a392426096c2ff4f76aa1547",
            "value": 1
          }
        },
        "e92b3187acda44e9b30051a9e410f43f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b1763f6b7fc9430ab2f773279fdd954d",
              "IPY_MODEL_145b34f60df1429e8685bcc9b2f05be2",
              "IPY_MODEL_fc589f87f65d4a3196cdd790bbdf7fa0"
            ],
            "layout": "IPY_MODEL_c0929c9ec77a48df8a7fd3e2f4539a51"
          }
        },
        "ec97a156272345ba8d456c5f090d9412": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4367d9584aa42e0b4249b33af0435d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4da859c892b42cfa6c909e7dfe2df7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d1eef416f5944c2911154fb770d869f",
            "placeholder": "​",
            "style": "IPY_MODEL_a3164d72e2d5431dbd7921b74290193a",
            "value": "100%"
          }
        },
        "fc589f87f65d4a3196cdd790bbdf7fa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3cd48beab0741b99a7885f89ebe1181",
            "placeholder": "​",
            "style": "IPY_MODEL_80837417e00b4fd3814524786898697c",
            "value": " 872/872 [00:00&lt;00:00, 9939.81 examples/s]"
          }
        },
        "state": {}
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
