{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8df29512-a5c0-4e06-8c53-91eaab143762",
   "metadata": {
    "editable": true,
    "id": "8df29512-a5c0-4e06-8c53-91eaab143762",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 第6章: 単語ベクトル"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05585fe-de49-4474-b2f9-57888e067319",
   "metadata": {
    "id": "b05585fe-de49-4474-b2f9-57888e067319"
   },
   "source": [
    "単語の意味を実ベクトルで表現する単語ベクトル（単語埋め込み）に関して、以下の処理を行うプログラムを作成せよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0afdc8e-57f5-4388-a914-1150d846d5da",
   "metadata": {
    "id": "c0afdc8e-57f5-4388-a914-1150d846d5da"
   },
   "source": [
    "## 50. 単語ベクトルの読み込みと表示\n",
    "\n",
    "Google Newsデータセット（約1,000億単語）での[学習済み単語ベクトル](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)（300万単語・フレーズ、300次元）をダウンロードし、\"United States\"の単語ベクトルを表示せよ。ただし、\"United States\"は内部的には\"United_States\"と表現されていることに注意せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a258ab02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.61328125e-02 -4.83398438e-02  2.35351562e-01  1.74804688e-01\n",
      " -1.46484375e-01 -7.42187500e-02 -1.01562500e-01 -7.71484375e-02\n",
      "  1.09375000e-01 -5.71289062e-02 -1.48437500e-01 -6.00585938e-02\n",
      "  1.74804688e-01 -7.71484375e-02  2.58789062e-02 -7.66601562e-02\n",
      " -3.80859375e-02  1.35742188e-01  3.75976562e-02 -4.19921875e-02\n",
      " -3.56445312e-02  5.34667969e-02  3.68118286e-04 -1.66992188e-01\n",
      " -1.17187500e-01  1.41601562e-01 -1.69921875e-01 -6.49414062e-02\n",
      " -1.66992188e-01  1.00585938e-01  1.15722656e-01 -2.18750000e-01\n",
      " -9.86328125e-02 -2.56347656e-02  1.23046875e-01 -3.54003906e-02\n",
      " -1.58203125e-01 -1.60156250e-01  2.94189453e-02  8.15429688e-02\n",
      "  6.88476562e-02  1.87500000e-01  6.49414062e-02  1.15234375e-01\n",
      " -2.27050781e-02  3.32031250e-01 -3.27148438e-02  1.77734375e-01\n",
      " -2.08007812e-01  4.54101562e-02 -1.23901367e-02  1.19628906e-01\n",
      "  7.44628906e-03 -9.03320312e-03  1.14257812e-01  1.69921875e-01\n",
      " -2.38281250e-01 -2.79541016e-02 -1.21093750e-01  2.47802734e-02\n",
      "  7.71484375e-02 -2.81982422e-02 -4.71191406e-02  1.78222656e-02\n",
      " -1.23046875e-01 -5.32226562e-02  2.68554688e-02 -3.11279297e-02\n",
      " -5.59082031e-02 -5.00488281e-02 -3.73535156e-02  1.25976562e-01\n",
      "  5.61523438e-02  1.51367188e-01  4.29687500e-02 -2.08007812e-01\n",
      " -4.78515625e-02  2.78320312e-02  1.81640625e-01  2.20703125e-01\n",
      " -3.61328125e-02 -8.39843750e-02 -3.69548798e-05 -9.52148438e-02\n",
      " -1.25000000e-01 -1.95312500e-01 -1.50390625e-01 -4.15039062e-02\n",
      "  1.31835938e-01  1.17675781e-01  1.91650391e-02  5.51757812e-02\n",
      " -9.42382812e-02 -1.08886719e-01  7.32421875e-02 -1.15234375e-01\n",
      "  8.93554688e-02 -1.40625000e-01  1.45507812e-01  4.49218750e-02\n",
      " -1.10473633e-02 -1.62353516e-02  4.05883789e-03  3.75976562e-02\n",
      " -6.98242188e-02 -5.46875000e-02  2.17285156e-02 -9.47265625e-02\n",
      "  4.24804688e-02  1.81884766e-02 -1.73339844e-02  4.63867188e-02\n",
      " -1.42578125e-01  1.99218750e-01  1.10839844e-01  2.58789062e-02\n",
      " -7.08007812e-02 -5.54199219e-02  3.45703125e-01  1.61132812e-01\n",
      " -2.44140625e-01 -2.59765625e-01 -9.71679688e-02  8.00781250e-02\n",
      " -8.78906250e-02 -7.22656250e-02  1.42578125e-01 -8.54492188e-02\n",
      " -3.18359375e-01  8.30078125e-02  6.34765625e-02  1.64062500e-01\n",
      " -1.92382812e-01 -1.17675781e-01 -5.41992188e-02 -1.56250000e-01\n",
      " -1.21582031e-01 -4.95605469e-02  1.20117188e-01 -3.83300781e-02\n",
      "  5.51757812e-02 -8.97216797e-03  4.32128906e-02  6.93359375e-02\n",
      "  8.93554688e-02  2.53906250e-01  1.65039062e-01  1.64062500e-01\n",
      " -1.41601562e-01  4.58984375e-02  1.97265625e-01 -8.98437500e-02\n",
      "  3.90625000e-02 -1.51367188e-01 -8.60595703e-03 -1.17675781e-01\n",
      " -1.97265625e-01 -1.12792969e-01  1.29882812e-01  1.96289062e-01\n",
      "  1.56402588e-03  3.93066406e-02  2.17773438e-01 -1.43554688e-01\n",
      "  6.03027344e-02 -1.35742188e-01  1.16210938e-01 -1.59912109e-02\n",
      "  2.79296875e-01  1.46484375e-01 -1.19628906e-01  1.76757812e-01\n",
      "  1.28906250e-01 -1.49414062e-01  6.93359375e-02 -1.72851562e-01\n",
      "  9.22851562e-02  1.33056641e-02 -2.00195312e-01 -9.76562500e-02\n",
      " -1.65039062e-01 -2.46093750e-01 -2.35595703e-02 -2.11914062e-01\n",
      "  1.84570312e-01 -1.85546875e-02  2.16796875e-01  5.05371094e-02\n",
      "  2.02636719e-02  4.25781250e-01  1.28906250e-01 -2.77099609e-02\n",
      "  1.29882812e-01 -1.15722656e-01 -2.05078125e-02  1.49414062e-01\n",
      "  7.81250000e-03 -2.05078125e-01 -8.05664062e-02 -2.67578125e-01\n",
      " -2.29492188e-02 -8.20312500e-02  8.64257812e-02  7.61718750e-02\n",
      " -3.66210938e-02  5.22460938e-02 -1.22070312e-01 -1.44042969e-02\n",
      " -2.69531250e-01  8.44726562e-02 -2.52685547e-02 -2.96630859e-02\n",
      " -1.68945312e-01  1.93359375e-01 -1.08398438e-01  1.94091797e-02\n",
      " -1.80664062e-01  1.93359375e-01 -7.08007812e-02  5.85937500e-02\n",
      " -1.01562500e-01 -1.31835938e-01  7.51953125e-02 -7.66601562e-02\n",
      "  3.37219238e-03 -8.59375000e-02  1.25000000e-01  2.92968750e-02\n",
      "  1.70898438e-01 -9.37500000e-02 -1.09375000e-01 -2.50244141e-02\n",
      "  2.11914062e-01 -4.44335938e-02  6.12792969e-02  2.62451172e-02\n",
      " -1.77734375e-01  1.23046875e-01 -7.42187500e-02 -1.67968750e-01\n",
      " -1.08886719e-01 -9.04083252e-04 -7.37304688e-02  5.49316406e-02\n",
      "  6.03027344e-02  8.39843750e-02  9.17968750e-02 -1.32812500e-01\n",
      "  1.22070312e-01 -8.78906250e-03  1.19140625e-01 -1.94335938e-01\n",
      " -6.64062500e-02 -2.07031250e-01  7.37304688e-02  8.93554688e-02\n",
      "  1.81884766e-02 -1.20605469e-01 -2.61230469e-02  2.67333984e-02\n",
      "  7.76367188e-02 -8.30078125e-02  6.78710938e-02 -3.54003906e-02\n",
      "  3.10546875e-01 -2.42919922e-02 -1.41601562e-01 -2.08007812e-01\n",
      " -4.57763672e-03 -6.54296875e-02 -4.95605469e-02  2.22656250e-01\n",
      "  1.53320312e-01 -1.38671875e-01 -5.24902344e-02  4.24804688e-02\n",
      " -2.38281250e-01  1.56250000e-01  5.83648682e-04 -1.20605469e-01\n",
      " -9.22851562e-02 -4.44335938e-02  3.61328125e-02 -1.86767578e-02\n",
      " -8.25195312e-02 -8.25195312e-02 -4.05273438e-02  1.19018555e-02\n",
      "  1.69921875e-01 -2.80761719e-02  3.03649902e-03  9.32617188e-02\n",
      " -8.49609375e-02  1.57470703e-02  7.03125000e-02  1.62353516e-02\n",
      " -2.27050781e-02  3.51562500e-02  2.47070312e-01 -2.67333984e-02]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    \"data\\GoogleNews-vectors-negative300.bin\",\n",
    "    binary=True,\n",
    ")\n",
    "\n",
    "print(model[\"United_States\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e3bb10-c37f-4395-9397-ac71e36bf4ed",
   "metadata": {
    "id": "c4e3bb10-c37f-4395-9397-ac71e36bf4ed"
   },
   "source": [
    "## 51. 単語の類似度\n",
    "\n",
    "\"United States\"と\"U.S.\"のコサイン類似度を計算せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec2af33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.7311\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gensim\n",
    "from torch.nn.functional import cosine_similarity as torch_cosine_similarity\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    \"data\\GoogleNews-vectors-negative300.bin\", binary=True\n",
    ")\n",
    "\n",
    "vec1 = torch.from_numpy(model[\"United_States\"])\n",
    "vec2 = torch.from_numpy(model[\"U.S.\"])\n",
    "\n",
    "# 次元を合わせて類似度を計算（1次元Tensor → 2次元バッチ）\n",
    "similarity = torch_cosine_similarity(vec1.unsqueeze(0), vec2.unsqueeze(0)).item()\n",
    "\n",
    "print(f\"Cosine similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36314a02-d3d8-4121-b310-e800b2d1ce3e",
   "metadata": {
    "id": "36314a02-d3d8-4121-b310-e800b2d1ce3e"
   },
   "source": [
    "## 52. 類似度の高い単語10件\n",
    "\n",
    "\"United States\"とコサイン類似度が高い10語と、その類似度を出力せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b1e261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Unites_States', 0.7877248525619507),\n",
      " ('Untied_States', 0.7541370987892151),\n",
      " ('United_Sates', 0.7400724291801453),\n",
      " ('U.S.', 0.7310774326324463),\n",
      " ('theUnited_States', 0.6404393911361694),\n",
      " ('America', 0.6178410053253174),\n",
      " ('UnitedStates', 0.6167312264442444),\n",
      " ('Europe', 0.6132988929748535),\n",
      " ('countries', 0.6044804453849792),\n",
      " ('Canada', 0.601906955242157)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import torch\n",
    "import gensim\n",
    "from torch.nn.functional import cosine_similarity as torch_cosine_similarity\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    \"data\\GoogleNews-vectors-negative300.bin\", binary=True\n",
    ")\n",
    "\n",
    "similarity_list = model.most_similar(\"United_States\", topn=10)\n",
    "pprint(similarity_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de3ff5c-5285-4e96-b8d7-4f995635d146",
   "metadata": {
    "id": "6de3ff5c-5285-4e96-b8d7-4f995635d146"
   },
   "source": [
    "## 53. 加法構成性によるアナロジー\n",
    "\n",
    "\"Spain\"の単語ベクトルから\"Madrid\"のベクトルを引き、\"Athens\"のベクトルを足したベクトルを計算し、そのベクトルと類似度の高い10語とその類似度を出力せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d93b348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Greece', 0.6898480653762817),\n",
      " ('Aristeidis_Grigoriadis', 0.560684859752655),\n",
      " ('Ioannis_Drymonakos', 0.5552908778190613),\n",
      " ('Greeks', 0.545068621635437),\n",
      " ('Ioannis_Christou', 0.5400862097740173),\n",
      " ('Hrysopiyi_Devetzi', 0.5248445272445679),\n",
      " ('Heraklio', 0.5207759737968445),\n",
      " ('Athens_Greece', 0.516880989074707),\n",
      " ('Lithuania', 0.5166865587234497),\n",
      " ('Iraklion', 0.5146791338920593)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import torch\n",
    "import gensim\n",
    "from torch.nn.functional import cosine_similarity as torch_cosine_similarity\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    \"data\\GoogleNews-vectors-negative300.bin\", binary=True\n",
    ")\n",
    "\n",
    "similarity_list = model.most_similar(\n",
    "    positive=[\"Spain\", \"Athens\"], negative=[\"Madrid\"], topn=10\n",
    ")\n",
    "pprint(similarity_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5db38ad-b7f4-40b2-a068-0dea071d23e7",
   "metadata": {
    "id": "d5db38ad-b7f4-40b2-a068-0dea071d23e7"
   },
   "source": [
    "## 54. アナロジーデータでの実験\n",
    "\n",
    "[単語アナロジーの評価データ](http://download.tensorflow.org/data/questions-words.txt)をダウンロードし、国と首都に関する事例（`: capital-common-countries`セクション）に対して、vec(2列目の単語) - vec(1列目の単語) + vec(3列目の単語)を計算し、そのベクトルと類似度が最も高い単語と、その類似度を求めよ。求めた単語と類似度は、各事例と一緒に記録せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "738f6aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19558/19558 [49:35<00:00,  6.57it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* capital-common-countries (506件)\n",
      "  Japan - Tokyo + Moscow -> Russia (sim=0.886, correct: Russia)\n",
      "  Russia - Moscow + Tehran -> Iran (sim=0.878, correct: Iran)\n",
      "  Russia - Moscow + Tokyo -> Japan (sim=0.877, correct: Japan)\n",
      "  Russia - Moscow + Beijing -> China (sim=0.869, correct: China)\n",
      "  Cuba - Havana + Tehran -> Iran (sim=0.864, correct: Iran)\n",
      "  ...\n",
      "  Japan - Tokyo + Bern -> Switzerland (sim=0.483, correct: Switzerland)\n",
      "  China - Beijing + Bern -> Bern_NC (sim=0.466, correct: Switzerland)\n",
      "  England - London + Bern -> Hanover (sim=0.453, correct: Switzerland)\n",
      "  Iraq - Baghdad + Bern -> coach_Bobby_Curlings (sim=0.435, correct: Switzerland)\n",
      "  Afghanistan - Kabul + Bern -> Bern_NC (sim=0.423, correct: Switzerland)\n",
      "\n",
      "* capital-world (4524件)\n",
      "  Ukraine - Kiev + Moscow -> Russia (sim=0.888, correct: Russia)\n",
      "  Russia - Moscow + Tehran -> Iran (sim=0.878, correct: Iran)\n",
      "  Russia - Moscow + Tokyo -> Japan (sim=0.877, correct: Japan)\n",
      "  Armenia - Yerevan + Baku -> Azerbaijan (sim=0.867, correct: Azerbaijan)\n",
      "  Philippines - Manila + Moscow -> Russia (sim=0.861, correct: Russia)\n",
      "  ...\n",
      "  Madagascar - Antananarivo + Bern -> coach_Bobby_Curlings (sim=0.420, correct: Switzerland)\n",
      "  Jordan - Amman + Antananarivo -> Mohamed_Berte (sim=0.413, correct: Madagascar)\n",
      "  Tuvalu - Funafuti + Kingston -> Jamaica (sim=0.407, correct: Jamaica)\n",
      "  Libya - Tripoli + Bern -> Switzerland (sim=0.402, correct: Switzerland)\n",
      "  Greenland - Nuuk + Algiers -> Gulf (sim=0.390, correct: Algeria)\n",
      "\n",
      "* currency (866件)\n",
      "  baht - Thailand + Malaysia -> RM## (sim=0.822, correct: ringgit)\n",
      "  ringgit - Malaysia + Thailand -> baht (sim=0.820, correct: baht)\n",
      "  baht - Thailand + Bulgaria -> leva (sim=0.814, correct: lev)\n",
      "  baht - Thailand + Nigeria -> naira (sim=0.811, correct: naira)\n",
      "  ringgit - Malaysia + Nigeria -> naira (sim=0.806, correct: naira)\n",
      "  ...\n",
      "  dong - Vietnam + USA -> lifts_Squaw_Valley (sim=0.376, correct: dollar)\n",
      "  lev - Bulgaria + USA -> lifts_Squaw_Valley (sim=0.376, correct: dollar)\n",
      "  riel - Cambodia + USA -> lifts_Squaw_Valley (sim=0.374, correct: dollar)\n",
      "  dram - Armenia + USA -> Tennent_lager (sim=0.373, correct: dollar)\n",
      "  rial - Iran + USA -> V_Singh_Fij (sim=0.346, correct: dollar)\n",
      "\n",
      "* city-in-state (2467件)\n",
      "  Nebraska - Omaha + Wichita -> Kansas (sim=0.825, correct: Kansas)\n",
      "  Michigan - Detroit + Milwaukee -> Wisconsin (sim=0.824, correct: Wisconsin)\n",
      "  Illinois - Chicago + Louisville -> Kentucky (sim=0.819, correct: Kentucky)\n",
      "  Florida - Tampa + Honolulu -> Hawaii (sim=0.817, correct: Hawaii)\n",
      "  Hawaii - Honolulu + Anchorage -> Alaska (sim=0.809, correct: Alaska)\n",
      "  ...\n",
      "  Washington - Spokane + Mesa -> Piñon (sim=0.424, correct: Arizona)\n",
      "  Georgia - Atlanta + Irving -> Marilyn_Flax_whose (sim=0.420, correct: Texas)\n",
      "  Washington - Tacoma + Mesa -> Arizona (sim=0.420, correct: Arizona)\n",
      "  Washington - Tacoma + Fontana -> WUSM_SL (sim=0.392, correct: California)\n",
      "  Washington - Spokane + Fontana -> Gus_Guida (sim=0.385, correct: California)\n",
      "\n",
      "* family (506件)\n",
      "  her - his + he -> she (sim=0.944, correct: she)\n",
      "  she - he + his -> her (sim=0.941, correct: her)\n",
      "  granddaughter - grandson + son -> daughter (sim=0.937, correct: daughter)\n",
      "  daughter - son + grandson -> granddaughter (sim=0.937, correct: granddaughter)\n",
      "  daughters - sons + son -> daughter (sim=0.932, correct: daughter)\n",
      "  ...\n",
      "  mom - dad + stepbrother -> mother (sim=0.548, correct: stepsister)\n",
      "  policewoman - policeman + stepbrother -> stepfather (sim=0.546, correct: stepsister)\n",
      "  stepsister - stepbrother + groom -> bride (sim=0.544, correct: bride)\n",
      "  stepdaughter - stepson + stepbrother -> niece (sim=0.542, correct: stepsister)\n",
      "  stepmother - stepfather + stepbrother -> sister (sim=0.521, correct: stepsister)\n",
      "\n",
      "* gram1-adjective-to-adverb (992件)\n",
      "  quickly - quick + swift -> swiftly (sim=0.770, correct: swiftly)\n",
      "  fortunately - fortunate + lucky -> luckily (sim=0.767, correct: luckily)\n",
      "  quickly - quick + rapid -> rapidly (sim=0.762, correct: rapidly)\n",
      "  luckily - lucky + fortunate -> fortunately (sim=0.760, correct: fortunately)\n",
      "  usually - usual + typical -> typically (sim=0.753, correct: typically)\n",
      "  ...\n",
      "  happily - happy + possible -> humanly_possible (sim=0.390, correct: possibly)\n",
      "  freely - free + possible -> deliberatively (sim=0.385, correct: possibly)\n",
      "  mostly - most + possible -> mainly (sim=0.376, correct: possibly)\n",
      "  freely - free + immediate -> immediately (sim=0.373, correct: immediately)\n",
      "  furiously - furious + immediate -> frantically (sim=0.368, correct: immediately)\n",
      "\n",
      "* gram2-opposite (812件)\n",
      "  inefficient - efficient + productive -> unproductive (sim=0.738, correct: unproductive)\n",
      "  uncomfortable - comfortable + pleasant -> unpleasant (sim=0.737, correct: unpleasant)\n",
      "  illogical - logical + rational -> irrational (sim=0.734, correct: irrational)\n",
      "  unreasonable - reasonable + rational -> irrational (sim=0.722, correct: irrational)\n",
      "  unsure - sure + aware -> unaware (sim=0.714, correct: unaware)\n",
      "  ...\n",
      "  impossible - possible + informed -> informing (sim=0.390, correct: uninformed)\n",
      "  impossibly - possibly + responsible -> solely_responsible (sim=0.389, correct: irresponsible)\n",
      "  undecided - decided + possible -> persuadable (sim=0.387, correct: impossible)\n",
      "  uninformative - informative + certain -> predetermined (sim=0.387, correct: uncertain)\n",
      "  uninformative - informative + possible -> humanly_possible (sim=0.387, correct: impossible)\n",
      "\n",
      "* gram3-comparative (1332件)\n",
      "  larger - large + big -> bigger (sim=0.848, correct: bigger)\n",
      "  stronger - strong + hard -> harder (sim=0.845, correct: harder)\n",
      "  tighter - tight + tough -> tougher (sim=0.840, correct: tougher)\n",
      "  harder - hard + tough -> tougher (sim=0.834, correct: tougher)\n",
      "  bigger - big + large -> larger (sim=0.834, correct: larger)\n",
      "  ...\n",
      "  worse - bad + short -> shorter (sim=0.458, correct: shorter)\n",
      "  newer - new + low -> high (sim=0.453, correct: lower)\n",
      "  worse - bad + new -> thenew (sim=0.452, correct: newer)\n",
      "  greater - great + old -> yearold (sim=0.427, correct: older)\n",
      "  newer - new + wide -> scatter_bomblets (sim=0.397, correct: wider)\n",
      "\n",
      "* gram4-superlative (1122件)\n",
      "  strangest - strange + weird -> weirdest (sim=0.827, correct: weirdest)\n",
      "  weirdest - weird + strange -> strangest (sim=0.824, correct: strangest)\n",
      "  strongest - strong + sharp -> sharpest (sim=0.818, correct: sharpest)\n",
      "  lowest - low + high -> highest (sim=0.814, correct: highest)\n",
      "  highest - high + low -> lowest (sim=0.805, correct: lowest)\n",
      "  ...\n",
      "  tallest - tall + short -> shortest (sim=0.455, correct: shortest)\n",
      "  oldest - old + fast -> fastest (sim=0.449, correct: fastest)\n",
      "  warmest - warm + quick -> quickest (sim=0.444, correct: quickest)\n",
      "  youngest - young + quick -> quickest (sim=0.444, correct: quickest)\n",
      "  oldest - old + quick -> easiest (sim=0.426, correct: quickest)\n",
      "\n",
      "* gram5-present-participle (1056件)\n",
      "  decreasing - decrease + increase -> increasing (sim=0.885, correct: increasing)\n",
      "  implementing - implement + enhance -> enhancing (sim=0.878, correct: enhancing)\n",
      "  increasing - increase + decrease -> decreasing (sim=0.874, correct: decreasing)\n",
      "  singing - sing + swim -> swimming (sim=0.869, correct: swimming)\n",
      "  enhancing - enhance + generate -> generating (sim=0.860, correct: generating)\n",
      "  ...\n",
      "  thinking - think + go -> Going (sim=0.471, correct: going)\n",
      "  coding - code + look -> looking (sim=0.461, correct: looking)\n",
      "  thinking - think + say -> Say (sim=0.447, correct: saying)\n",
      "  coding - code + go -> goes (sim=0.443, correct: going)\n",
      "  saying - say + look -> looks (sim=0.440, correct: looking)\n",
      "\n",
      "* gram6-nationality-adjective (1599件)\n",
      "  Japanese - Japan + China -> Chinese (sim=0.928, correct: Chinese)\n",
      "  Chinese - China + Japan -> Japanese (sim=0.927, correct: Japanese)\n",
      "  German - Germany + Italy -> Italian (sim=0.926, correct: Italian)\n",
      "  Russian - Russia + Bulgaria -> Bulgarian (sim=0.919, correct: Bulgarian)\n",
      "  Italian - Italy + Germany -> German (sim=0.917, correct: German)\n",
      "  ...\n",
      "  English - England + Albania -> Macedonian (sim=0.534, correct: Albanian)\n",
      "  English - England + Austria -> Austrian (sim=0.533, correct: Austrian)\n",
      "  Albanian - Albania + England -> stock_symbol_BNK (sim=0.530, correct: English)\n",
      "  Argentinean - Argentina + England -> ticker_symbol_BNK (sim=0.514, correct: English)\n",
      "  Belorussian - Belarus + England -> stock_symbol_BNK (sim=0.514, correct: English)\n",
      "\n",
      "* gram7-past-tense (1560件)\n",
      "  increased - increasing + decreasing -> decreased (sim=0.900, correct: decreased)\n",
      "  decreased - decreasing + increasing -> increased (sim=0.886, correct: increased)\n",
      "  danced - dancing + singing -> sang (sim=0.866, correct: sang)\n",
      "  saw - seeing + taking -> took (sim=0.848, correct: took)\n",
      "  sang - singing + screaming -> screamed (sim=0.847, correct: screamed)\n",
      "  ...\n",
      "  fed - feeding + saying -> implying (sim=0.459, correct: said)\n",
      "  went - going + knowing -> hid (sim=0.458, correct: knew)\n",
      "  decreased - decreasing + saying -> stating (sim=0.457, correct: said)\n",
      "  implemented - implementing + saying -> stating (sim=0.455, correct: said)\n",
      "  spent - spending + looking -> look (sim=0.443, correct: looked)\n",
      "\n",
      "* gram8-plural (1332件)\n",
      "  horses - horse + dog -> dogs (sim=0.931, correct: dogs)\n",
      "  dogs - dog + horse -> horses (sim=0.926, correct: horses)\n",
      "  cats - cat + dog -> dogs (sim=0.909, correct: dogs)\n",
      "  dogs - dog + cat -> cats (sim=0.898, correct: cats)\n",
      "  cats - cat + horse -> horses (sim=0.883, correct: horses)\n",
      "  ...\n",
      "  monkeys - monkey + hand -> hands (sim=0.449, correct: hands)\n",
      "  dollars - dollar + lion -> lions (sim=0.445, correct: lions)\n",
      "  pineapples - pineapple + hand -> hands (sim=0.432, correct: hands)\n",
      "  dollars - dollar + mouse -> Logitech_MX_Revolution (sim=0.432, correct: mice)\n",
      "  mice - mouse + hand -> hands (sim=0.429, correct: hands)\n",
      "\n",
      "* gram9-plural-verbs (870件)\n",
      "  increases - increase + decrease -> decreases (sim=0.878, correct: decreases)\n",
      "  decreases - decrease + increase -> increases (sim=0.878, correct: increases)\n",
      "  provides - provide + generate -> generates (sim=0.862, correct: generates)\n",
      "  generates - generate + provide -> provides (sim=0.852, correct: provides)\n",
      "  provides - provide + enhance -> enhances (sim=0.842, correct: enhances)\n",
      "  ...\n",
      "  implements - implement + say -> argue (sim=0.472, correct: says)\n",
      "  talks - talk + see -> negotiations (sim=0.471, correct: sees)\n",
      "  listens - listen + say -> believe (sim=0.464, correct: says)\n",
      "  works - work + say -> argue (sim=0.458, correct: says)\n",
      "  talks - talk + describe -> describing (sim=0.448, correct: describes)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import torch\n",
    "import numpy as np\n",
    "import gensim\n",
    "from tqdm import tqdm  \n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    \"data/GoogleNews-vectors-negative300.bin\", binary=True\n",
    ")\n",
    "\n",
    "analogies = dict()\n",
    "buff = []\n",
    "key = None\n",
    "\n",
    "with open(\"data/questions-words.txt\") as f:\n",
    "    total_lines = sum(1 for _ in f)\n",
    "\n",
    "with open(\"data/questions-words.txt\") as f:\n",
    "    for line in tqdm(f.readlines(), total=total_lines):\n",
    "        ws = line.strip().split()\n",
    "        if not ws:  # 空行をスキップ\n",
    "            continue\n",
    "\n",
    "        if ws[0] == \":\":\n",
    "            if key:\n",
    "                analogies[key] = buff\n",
    "                buff = []\n",
    "            key = ws[1]\n",
    "        else:\n",
    "            try:\n",
    "                v1 = model[ws[0]]\n",
    "                v2 = model[ws[1]]\n",
    "                v3 = model[ws[2]]\n",
    "\n",
    "                # 正規化\n",
    "                v1 = v1 / np.linalg.norm(v1)\n",
    "                v2 = v2 / np.linalg.norm(v2)\n",
    "                v3 = v3 / np.linalg.norm(v3)\n",
    "\n",
    "                v0 = v2 - v1 + v3\n",
    "                \n",
    "                rst = model.most_similar(\n",
    "                    positive=[ws[1], ws[2]], negative=[ws[0]], topn=1\n",
    "                )\n",
    "                pred, sim = rst[0]\n",
    "                buff.append([*ws, v0, pred, sim])\n",
    "            except KeyError as e:\n",
    "                # 単語がモデルに存在しない場合はスキップ\n",
    "                print(f\"KeyError: {e}\")\n",
    "\n",
    "    # 最後のカテゴリを追加\n",
    "    if key:\n",
    "        analogies[key] = buff\n",
    "\n",
    "for key, vals in analogies.items():\n",
    "    print(f\"* {key} ({len(vals)}件)\")\n",
    "\n",
    "    if not vals:\n",
    "        print(\"  データなし\")\n",
    "        continue\n",
    "\n",
    "    vals_sorted = sorted(vals, key=lambda v: v[6], reverse=True)  # 類似度でソーティング\n",
    "\n",
    "    for v in vals_sorted[:5]:\n",
    "        print(f\"  {v[1]} - {v[0]} + {v[2]} -> {v[5]} (sim={v[6]:.3f}, correct: {v[3]})\")\n",
    "\n",
    "    if len(vals_sorted) > 10:\n",
    "        print(\"  ...\")\n",
    "\n",
    "    for v in vals_sorted[-5:]:\n",
    "        print(f\"  {v[1]} - {v[0]} + {v[2]} -> {v[5]} (sim={v[6]:.3f}, correct: {v[3]})\")\n",
    "\n",
    "    print() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc1fcff-5555-44da-8b4c-12d97ede10e8",
   "metadata": {
    "id": "dbc1fcff-5555-44da-8b4c-12d97ede10e8"
   },
   "source": [
    "## 55. アナロジータスクでの正解率\n",
    "\n",
    "54の実行結果を用い、意味的アナロジー（semantic analogy）と文法的アナロジー（syntactic analogy）の正解率を測定せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6da90504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for semantic analogies: 0.7308602999210734\n",
      "accuracy for syntactic analogies: 0.7400468384074942\n"
     ]
    }
   ],
   "source": [
    "cor_sem = 0  # 意味的アナロジーの正解数\n",
    "tot_sem = 0  # 意味的アナロジーの総数\n",
    "cor_syn = 0  # 文法的アナロジーの正解数\n",
    "tot_syn = 0  # 文法的アナロジーの総数\n",
    "\n",
    "for key, vals in analogies.items():\n",
    "    if key[:4] == \"gram\":\n",
    "        tot_syn += len(vals)\n",
    "        cor_syn += len(list(filter(lambda v: v[3] == v[5], vals)))\n",
    "    else:\n",
    "        tot_sem += len(vals)\n",
    "        cor_sem += len(list(filter(lambda v: v[3] == v[5], vals)))\n",
    "\n",
    "print(f\"accuracy for semantic analogies: {cor_sem/tot_sem}\")\n",
    "print(f\"accuracy for syntactic analogies: {cor_syn/tot_syn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62a9077-45e9-4cf2-8546-5e5b226b4cb6",
   "metadata": {
    "id": "b62a9077-45e9-4cf2-8546-5e5b226b4cb6"
   },
   "source": [
    "## 56. WordSimilarity-353での評価\n",
    "\n",
    "[The WordSimilarity-353 Test Collection](http://www.gabrilovich.com/resources/data/wordsim353/wordsim353.html)の評価データをダウンロードし、単語ベクトルにより計算される類似度のランキングと、人間の類似度判定のランキングの間のスピアマン相関係数を計算せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd06aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e45f26a-5896-453b-a11b-a97ef3b3900c",
   "metadata": {
    "id": "4e45f26a-5896-453b-a11b-a97ef3b3900c"
   },
   "source": [
    "## 57. k-meansクラスタリング\n",
    "\n",
    "国名に関する単語ベクトルを抽出し、k-meansクラスタリングをクラスタ数k=5として実行せよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63cb16e-7cbe-49e8-8f66-d703600128fa",
   "metadata": {
    "id": "c63cb16e-7cbe-49e8-8f66-d703600128fa"
   },
   "source": [
    "## 58. Ward法によるクラスタリング\n",
    "\n",
    "国名に関する単語ベクトルに対し、Ward法による階層型クラスタリングを実行せよ。さらに、クラスタリング結果をデンドログラムとして可視化せよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "epeDPNhuFoEY",
   "metadata": {
    "id": "epeDPNhuFoEY"
   },
   "source": [
    "## 59. t-SNEによる可視化\n",
    "\n",
    "ベクトル空間上の国名に関する単語ベクトルをt-SNEで可視化せよ。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
