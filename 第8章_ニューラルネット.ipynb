{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e1YwuFtZd1t",
   "metadata": {
    "editable": true,
    "id": "1e1YwuFtZd1t",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 第8章: ニューラルネット\n",
    "\n",
    "第7章で取り組んだポジネガ分類を題材として、ニューラルネットワークで分類モデルを実装する。なお、この章ではPyTorchやTensorFlow、JAXなどの深層学習フレームワークを活用せよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03603ee-a54b-4b93-97a2-888f5e3feeff",
   "metadata": {
    "id": "b03603ee-a54b-4b93-97a2-888f5e3feeff"
   },
   "source": [
    "## 70. 単語埋め込みの読み込み\n",
    "\n",
    "事前学習済み単語埋め込みを活用し、$|V| \\times d_\\rm{emb}$ の単語埋め込み行列$\\pmb{E}$を作成せよ。ここで、$|V|$は単語埋め込みの語彙数、$d_\\rm{emb}$は単語埋め込みの次元数である。ただし、単語埋め込み行列の先頭の行ベクトル$\\pmb{E}_{0,:}$は、将来的にパディング（`<PAD>`）トークンの埋め込みベクトルとして用いたいので、ゼロベクトルとして予約せよ。ゆえに、$\\pmb{E}$の2行目以降に事前学習済み単語埋め込みを読み込むことになる。\n",
    "\n",
    "もし、Google Newsデータセットの[学習済み単語ベクトル](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)（300万単語・フレーズ、300次元）を全て読み込んだ場合、$|V|=3000001, d_\\rm{emb}=300$になるはずである（ただ、300万単語の中には、殆ど用いられない稀な単語も含まれるので、語彙を削減した方がメモリの節約になる）。\n",
    "\n",
    "また、単語埋め込み行列の構築と同時に、単語埋め込み行列の各行のインデックス番号（トークンID）と、単語（トークン）への双方向の対応付けを保持せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88cb001d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (3000001, 300)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    \"data\\GoogleNews-vectors-negative300.bin\",\n",
    "    binary=True,\n",
    ")\n",
    "\n",
    "vocab_size = len(model.key_to_index) + 1\n",
    "embedding_dim = model.vector_size\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
    "\n",
    "token2id = {\"<pad>\": 0}\n",
    "id2token = {0: \"<pad>\"}\n",
    "\n",
    "for idx, word in enumerate(model.key_to_index):\n",
    "    token2id[word] = idx + 1\n",
    "    id2token[idx + 1] = word\n",
    "    embedding_matrix[idx + 1] = model[word]\n",
    "\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45bc5ba-4a83-493a-a78e-04aa48f3db2e",
   "metadata": {
    "id": "c45bc5ba-4a83-493a-a78e-04aa48f3db2e"
   },
   "source": [
    "## 71. データセットの読み込み\n",
    "\n",
    "[General Language Understanding Evaluation (GLUE)](https://gluebenchmark.com/) ベンチマークで配布されている[Stanford Sentiment Treebank (SST)](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip) をダウンロードし、訓練セット（train.tsv）と開発セット（dev.tsv）のテキストと極性ラベルと読み込み、全てのテキストをトークンID列に変換せよ。このとき、単語埋め込みの語彙でカバーされていない単語は無視し、トークン列に含めないことにせよ。また、テキストの全トークンが単語埋め込みの語彙に含まれておらず、空のトークン列となってしまう事例は、訓練セットおよび開発セットから削除せよ（このため、第7章の実験で得られた正解率と比較できなくなることに注意せよ）。\n",
    "\n",
    "事例の表現方法は任意でよいが、例えば\"contains no wit , only labored gags\"がネガティブに分類される事例は、次のような辞書オブジェクトで表現すればよい。\n",
    "\n",
    "```\n",
    "{'text': 'contains no wit , only labored gags',\n",
    " 'label': tensor([0.]),\n",
    " 'input_ids': tensor([ 3475,    87, 15888,    90, 27695, 42637])}\n",
    "```\n",
    "\n",
    "この例では、`text`はテキスト、`label`は分類ラベル（ポジティブなら`tensor([1.])`、ネガティブなら`tensor([0.])`）、`input_ids`はテキストのトークン列をID列で表現している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d8c9b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "# データセットのダウンロード\n",
    "url = \"https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\"\n",
    "zip_path = \"./data/SST-2.zip\"\n",
    "\n",
    "response = requests.get(url)\n",
    "with open(zip_path, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "    \n",
    "# ZIPファイルを解凍\n",
    "with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfc6ec58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 66650\n",
      "Dev dataset size: 872\n",
      "\n",
      "Example of processed data:\n",
      "{'text': 'hide new secretions from the parental units ', 'label': tensor([0.]), 'input_ids': tensor([  5785,     66, 113845,     18,     12,  15095,   1594])}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "train_path = \"./data/SST-2/train.tsv\"\n",
    "dev_path = \"./data/SST-2/dev.tsv\"\n",
    "\n",
    "def load_data(filepath):\n",
    "    df = pd.read_csv(filepath, sep=\"\\t\", header=0)\n",
    "    df.columns = [\"text\", \"label\"]\n",
    "    df[\"label\"] = df[\"label\"].astype(int)\n",
    "    return df\n",
    "\n",
    "def make_dict(df):\n",
    "    dataset = []\n",
    "    for _, row in df.iterrows():\n",
    "        text = row[\"text\"]\n",
    "        label = row[\"label\"]\n",
    "        \n",
    "        # テキストをトークン化し、IDに変換\n",
    "        words = text.split()\n",
    "        input_ids = []\n",
    "        \n",
    "        for word in words:\n",
    "            if word in token2id:\n",
    "                input_ids.append(token2id[word])\n",
    "        \n",
    "        # 空のトークン列の事例は無視する\n",
    "        if len(input_ids) > 0:\n",
    "            dataset.append({\n",
    "                \"text\": text,\n",
    "                \"label\": torch.tensor([float(label)]),\n",
    "                \"input_ids\": torch.tensor(input_ids)\n",
    "            })\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# データの読み込みと処理\n",
    "train_df = load_data(train_path)\n",
    "dev_df = load_data(dev_path)\n",
    "\n",
    "train_dataset = make_dict(train_df)\n",
    "dev_dataset = make_dict(dev_df)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Dev dataset size: {len(dev_dataset)}\")\n",
    "print(\"\\nExample:\")\n",
    "print(train_dataset[0])\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dfe527-a08c-48fa-b9b4-0acebea36bca",
   "metadata": {
    "id": "29dfe527-a08c-48fa-b9b4-0acebea36bca"
   },
   "source": [
    "## 72. Bag of wordsモデルの構築\n",
    "\n",
    "単語埋め込みの平均ベクトルでテキストの特徴ベクトルを表現し、重みベクトルとの内積でポジティブ及びネガティブを分類するニューラルネットワーク（ロジスティック回帰モデル）を設計せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7abb6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, embedding_matrix, freeze_embedding=True):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix, dtype=torch.float32),\n",
    "            freeze=freeze_embedding,\n",
    "        )\n",
    "        self.linear = nn.Linear(embedding_matrix.shape[1], 1)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        mean_embedded = embedded.mean(dim=0)\n",
    "        return self.linear(mean_embedded)\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LogisticRegression(embedding_matrix, True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72385c44-ceab-4d62-a4df-3023e15a37e2",
   "metadata": {
    "id": "72385c44-ceab-4d62-a4df-3023e15a37e2"
   },
   "source": [
    "## 73. モデルの学習\n",
    "\n",
    "問題72で設計したモデルの重みベクトルを訓練セット上で学習せよ。ただし、学習中は単語埋め込み行列の値を固定せよ（単語埋め込み行列のファインチューニングは行わない）。また、学習時に損失値を表示するなど、学習の進捗状況をモニタリングできるようにせよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cae9947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.6528\n",
      "Epoch 2/5, Loss: 0.5941\n",
      "Epoch 3/5, Loss: 0.5532\n",
      "Epoch 4/5, Loss: 0.5239\n",
      "Epoch 5/5, Loss: 0.5021\n"
     ]
    }
   ],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, embedding_matrix, freeze_embedding=True):\n",
    "        super().__init__()\n",
    "        # 事前学習済みの埋め込み行列から nn.Embedding を作成\n",
    "        # freeze=True なら重みを学習させない（凍結）\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix, dtype=torch.float32),\n",
    "            freeze=freeze_embedding,\n",
    "        )\n",
    "        # 線形層：入力は埋め込みの次元数 → 出力は1\n",
    "        self.linear = nn.Linear(embedding_matrix.shape[1], 1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # 入力（トークンID列）を埋め込みベクトルに変換\n",
    "        embedded = self.embedding(input_ids)  # [seq_len, emb_dim]\n",
    "        # 埋め込みベクトルの平均を計算\n",
    "        mean_embedded = embedded.mean(dim=0).unsqueeze(0)  # [1, emb_dim]\n",
    "        return self.linear(mean_embedded)  # [1, 1]\n",
    "\n",
    "model = LogisticRegression(embedding_matrix, True).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train(model, dataset, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for data in dataset:\n",
    "        input_ids = data[\"input_ids\"].to(device)\n",
    "        labels = data[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids)\n",
    "        # ラベルを [1] → [1, 1] に変形して損失を計算\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataset)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_dataset, optimizer, criterion, device)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"./model/No_73.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b25b5b-0ed2-4bf0-8350-601812eb057f",
   "metadata": {
    "id": "26b25b5b-0ed2-4bf0-8350-601812eb057f"
   },
   "source": [
    "## 74. モデルの評価\n",
    "\n",
    "問題73で学習したモデルの開発セットにおける正解率を求めよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1f1ef20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7615\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate(model, dataset, device):\n",
    "    model.eval() \n",
    "\n",
    "    y_preds = []  \n",
    "    y_trues = [] \n",
    "\n",
    "    for data in dataset:\n",
    "        input_ids = data[\"input_ids\"].to(device)\n",
    "        labels = data[\"label\"].to(device)\n",
    "\n",
    "        with torch.no_grad():  # 勾配計算を無効化（高速化・メモリ削減）\n",
    "            outputs = model(input_ids)  \n",
    "            probs = (\n",
    "                torch.sigmoid(outputs).squeeze().cpu().numpy()\n",
    "            )  # 確率に変換\n",
    "\n",
    "            y_preds.append(probs)\n",
    "            y_trues.append(labels.item())  # .item() で整数に変換\n",
    "\n",
    "    y_preds = np.array(y_preds)\n",
    "    y_trues = np.array(y_trues)\n",
    "\n",
    "    binary_preds = (y_preds > 0.5).astype(int)\n",
    "    accuracy = accuracy_score(y_trues, binary_preds)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "accuracy = evaluate(model, dev_dataset, device)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O08V9g0mcJwe",
   "metadata": {
    "id": "O08V9g0mcJwe"
   },
   "source": [
    "## 75. パディング\n",
    "\n",
    "複数の事例が与えられたとき、これらをまとめて一つのテンソル・オブジェクトで表現する関数`collate`を実装せよ。与えられた複数の事例のトークン列の長さが異なるときは、トークン列の長さが最も長いものに揃え、0番のトークンIDでパディングをせよ。さらに、トークン列の長さが長いものから順に、事例を並び替えよ。\n",
    "\n",
    "例えば、訓練データセットの冒頭の4事例が次のように表されているとき、\n",
    "\n",
    "```\n",
    "[{'text': 'hide new secretions from the parental units',\n",
    "  'label': tensor([0.]),\n",
    "  'input_ids': tensor([  5785,     66, 113845,     18,     12,  15095,   1594])},\n",
    " {'text': 'contains no wit , only labored gags',\n",
    "  'label': tensor([0.]),\n",
    "  'input_ids': tensor([ 3475,    87, 15888,    90, 27695, 42637])},\n",
    " {'text': 'that loves its characters and communicates something rather beautiful about human nature',\n",
    "  'label': tensor([1.]),\n",
    "  'input_ids': tensor([    4,  5053,    45,  3305, 31647,   348,   904,  2815,    47,  1276,  1964])},\n",
    " {'text': 'remains utterly satisfied to remain the same throughout',\n",
    "  'label': tensor([0.]),\n",
    "  'input_ids': tensor([  987, 14528,  4941,   873,    12,   208,   898])}]\n",
    "```\n",
    "\n",
    "`collate`関数を通した結果は以下のようになることが想定される。\n",
    "\n",
    "```\n",
    "{'input_ids': tensor([\n",
    "    [     4,   5053,     45,   3305,  31647,    348,    904,   2815,     47,   1276,   1964],\n",
    "    [  5785,     66, 113845,     18,     12,  15095,   1594,      0,      0,      0,      0],\n",
    "    [   987,  14528,   4941,    873,     12,    208,    898,      0,      0,      0,      0],\n",
    "    [  3475,     87,  15888,     90,  27695,  42637,      0,      0,      0,      0,      0]]),\n",
    " 'label': tensor([\n",
    "    [1.],\n",
    "    [0.],\n",
    "    [0.],\n",
    "    [0.]])}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9NzvuZ-5ebDU",
   "metadata": {
    "id": "9NzvuZ-5ebDU"
   },
   "source": [
    "## 76. ミニバッチ学習\n",
    "\n",
    "問題75のパディングの処理を活用して、ミニバッチでモデルを学習せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RUbjivUTejxn",
   "metadata": {
    "id": "RUbjivUTejxn"
   },
   "source": [
    "## 77. GPU上での学習\n",
    "\n",
    "問題76のモデル学習をGPU上で実行せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZUY1PsD-eplq",
   "metadata": {
    "id": "ZUY1PsD-eplq"
   },
   "source": [
    "## 78. 単語埋め込みのファインチューニング\n",
    "\n",
    "問題77の学習において、単語埋め込みのパラメータも同時に更新するファインチューニングを導入せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jVAdWIq0evKR",
   "metadata": {
    "id": "jVAdWIq0evKR"
   },
   "source": [
    "## 79. アーキテクチャの変更\n",
    "\n",
    "ニューラルネットワークのアーキテクチャを自由に変更し、モデルを学習せよ。また、学習したモデルの開発セットにおける正解率を求めよ。例えば、テキストの特徴ベクトル（単語埋め込みの平均ベクトル）に対して多層のニューラルネットワークを通したり、畳み込みニューラルネットワーク（CNN; Convolutional Neural Network）や再帰型ニューラルネットワーク（RNN; Recurrent Neural Network）などのモデルの学習に挑戦するとよい。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
