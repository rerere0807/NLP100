{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e1YwuFtZd1t",
   "metadata": {
    "editable": true,
    "id": "1e1YwuFtZd1t",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 第8章: ニューラルネット\n",
    "\n",
    "第7章で取り組んだポジネガ分類を題材として、ニューラルネットワークで分類モデルを実装する。なお、この章ではPyTorchやTensorFlow、JAXなどの深層学習フレームワークを活用せよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03603ee-a54b-4b93-97a2-888f5e3feeff",
   "metadata": {
    "id": "b03603ee-a54b-4b93-97a2-888f5e3feeff"
   },
   "source": [
    "## 70. 単語埋め込みの読み込み\n",
    "\n",
    "事前学習済み単語埋め込みを活用し、$|V| \\times d_\\rm{emb}$ の単語埋め込み行列$\\pmb{E}$を作成せよ。ここで、$|V|$は単語埋め込みの語彙数、$d_\\rm{emb}$は単語埋め込みの次元数である。ただし、単語埋め込み行列の先頭の行ベクトル$\\pmb{E}_{0,:}$は、将来的にパディング（`<PAD>`）トークンの埋め込みベクトルとして用いたいので、ゼロベクトルとして予約せよ。ゆえに、$\\pmb{E}$の2行目以降に事前学習済み単語埋め込みを読み込むことになる。\n",
    "\n",
    "もし、Google Newsデータセットの[学習済み単語ベクトル](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)（300万単語・フレーズ、300次元）を全て読み込んだ場合、$|V|=3000001, d_\\rm{emb}=300$になるはずである（ただ、300万単語の中には、殆ど用いられない稀な単語も含まれるので、語彙を削減した方がメモリの節約になる）。\n",
    "\n",
    "また、単語埋め込み行列の構築と同時に、単語埋め込み行列の各行のインデックス番号（トークンID）と、単語（トークン）への双方向の対応付けを保持せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88cb001d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (3000001, 300)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    \"data\\GoogleNews-vectors-negative300.bin\",\n",
    "    binary=True,\n",
    ")\n",
    "\n",
    "vocab_size = len(model.key_to_index) + 1\n",
    "embedding_dim = model.vector_size\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
    "\n",
    "token2id = {\"<pad>\": 0}\n",
    "id2token = {0: \"<pad>\"}\n",
    "\n",
    "for idx, word in enumerate(model.key_to_index):\n",
    "    token2id[word] = idx + 1\n",
    "    id2token[idx + 1] = word\n",
    "    embedding_matrix[idx + 1] = model[word]\n",
    "\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45bc5ba-4a83-493a-a78e-04aa48f3db2e",
   "metadata": {
    "id": "c45bc5ba-4a83-493a-a78e-04aa48f3db2e"
   },
   "source": [
    "## 71. データセットの読み込み\n",
    "\n",
    "[General Language Understanding Evaluation (GLUE)](https://gluebenchmark.com/) ベンチマークで配布されている[Stanford Sentiment Treebank (SST)](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip) をダウンロードし、訓練セット（train.tsv）と開発セット（dev.tsv）のテキストと極性ラベルと読み込み、全てのテキストをトークンID列に変換せよ。このとき、単語埋め込みの語彙でカバーされていない単語は無視し、トークン列に含めないことにせよ。また、テキストの全トークンが単語埋め込みの語彙に含まれておらず、空のトークン列となってしまう事例は、訓練セットおよび開発セットから削除せよ（このため、第7章の実験で得られた正解率と比較できなくなることに注意せよ）。\n",
    "\n",
    "事例の表現方法は任意でよいが、例えば\"contains no wit , only labored gags\"がネガティブに分類される事例は、次のような辞書オブジェクトで表現すればよい。\n",
    "\n",
    "```\n",
    "{'text': 'contains no wit , only labored gags',\n",
    " 'label': tensor([0.]),\n",
    " 'input_ids': tensor([ 3475,    87, 15888,    90, 27695, 42637])}\n",
    "```\n",
    "\n",
    "この例では、`text`はテキスト、`label`は分類ラベル（ポジティブなら`tensor([1.])`、ネガティブなら`tensor([0.])`）、`input_ids`はテキストのトークン列をID列で表現している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d8c9b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "# データセットのダウンロード\n",
    "url = \"https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\"\n",
    "zip_path = \"./data/SST-2.zip\"\n",
    "\n",
    "response = requests.get(url)\n",
    "with open(zip_path, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "    \n",
    "# ZIPファイルを解凍\n",
    "with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfc6ec58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 66650\n",
      "Dev dataset size: 872\n",
      "\n",
      "Example:\n",
      "{'text': 'hide new secretions from the parental units ', 'label': tensor([0.]), 'input_ids': tensor([  5785,     66, 113845,     18,     12,  15095,   1594])}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "train_path = \"./data/SST-2/train.tsv\"\n",
    "dev_path = \"./data/SST-2/dev.tsv\"\n",
    "\n",
    "def load_data(filepath):\n",
    "    df = pd.read_csv(filepath, sep=\"\\t\", header=0)\n",
    "    df.columns = [\"text\", \"label\"]\n",
    "    df[\"label\"] = df[\"label\"].astype(int)\n",
    "    return df\n",
    "\n",
    "def make_dict(df):\n",
    "    dataset = []\n",
    "    for _, row in df.iterrows():\n",
    "        text = row[\"text\"]\n",
    "        label = row[\"label\"]\n",
    "        \n",
    "        # テキストをトークン化し、IDに変換\n",
    "        words = text.split()\n",
    "        input_ids = []\n",
    "        \n",
    "        for word in words:\n",
    "            if word in token2id:\n",
    "                input_ids.append(token2id[word])\n",
    "        \n",
    "        # 空のトークン列の事例は無視する\n",
    "        if len(input_ids) > 0:\n",
    "            dataset.append({\n",
    "                \"text\": text,\n",
    "                \"label\": torch.tensor([float(label)]),\n",
    "                \"input_ids\": torch.tensor(input_ids)\n",
    "            })\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# データの読み込みと処理\n",
    "train_df = load_data(train_path)\n",
    "dev_df = load_data(dev_path)\n",
    "\n",
    "train_dataset = make_dict(train_df)\n",
    "dev_dataset = make_dict(dev_df)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Dev dataset size: {len(dev_dataset)}\")\n",
    "print(\"\\nExample:\")\n",
    "print(train_dataset[0])\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dfe527-a08c-48fa-b9b4-0acebea36bca",
   "metadata": {
    "id": "29dfe527-a08c-48fa-b9b4-0acebea36bca"
   },
   "source": [
    "## 72. Bag of wordsモデルの構築\n",
    "\n",
    "単語埋め込みの平均ベクトルでテキストの特徴ベクトルを表現し、重みベクトルとの内積でポジティブ及びネガティブを分類するニューラルネットワーク（ロジスティック回帰モデル）を設計せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7abb6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, embedding_matrix, freeze_embedding=True):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix, dtype=torch.float32),\n",
    "            freeze=freeze_embedding,\n",
    "        )\n",
    "        self.linear = nn.Linear(embedding_matrix.shape[1], 1)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        mean_embedded = embedded.mean(dim=0)\n",
    "        return self.linear(mean_embedded)\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LogisticRegression(embedding_matrix, True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72385c44-ceab-4d62-a4df-3023e15a37e2",
   "metadata": {
    "id": "72385c44-ceab-4d62-a4df-3023e15a37e2"
   },
   "source": [
    "## 73. モデルの学習\n",
    "\n",
    "問題72で設計したモデルの重みベクトルを訓練セット上で学習せよ。ただし、学習中は単語埋め込み行列の値を固定せよ（単語埋め込み行列のファインチューニングは行わない）。また、学習時に損失値を表示するなど、学習の進捗状況をモニタリングできるようにせよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cae9947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.6527\n",
      "Epoch 2/5, Loss: 0.5942\n",
      "Epoch 3/5, Loss: 0.5534\n",
      "Epoch 4/5, Loss: 0.5241\n",
      "Epoch 5/5, Loss: 0.5024\n"
     ]
    }
   ],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, embedding_matrix, freeze_embedding=True):\n",
    "        super().__init__()\n",
    "        # 事前学習済みの埋め込み行列から nn.Embedding を作成\n",
    "        # freeze=True なら重みを学習させない（凍結）\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix, dtype=torch.float32),\n",
    "            freeze=freeze_embedding,\n",
    "        )\n",
    "        # 線形層：入力は埋め込みの次元数 → 出力は1\n",
    "        self.linear = nn.Linear(embedding_matrix.shape[1], 1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # 入力（トークンID列）を埋め込みベクトルに変換\n",
    "        embedded = self.embedding(input_ids)  # [seq_len, emb_dim]\n",
    "        # 埋め込みベクトルの平均を計算\n",
    "        mean_embedded = embedded.mean(dim=0).unsqueeze(0)  # [1, emb_dim]\n",
    "        return self.linear(mean_embedded)  # [1, 1]\n",
    "\n",
    "model = LogisticRegression(embedding_matrix, True).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train(model, dataset, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for data in dataset:\n",
    "        input_ids = data[\"input_ids\"].to(device)\n",
    "        labels = data[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids)\n",
    "        # ラベルを [1] → [1, 1] に変形して損失を計算\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataset)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_dataset, optimizer, criterion, device)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"./model/No_73.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b25b5b-0ed2-4bf0-8350-601812eb057f",
   "metadata": {
    "id": "26b25b5b-0ed2-4bf0-8350-601812eb057f"
   },
   "source": [
    "## 74. モデルの評価\n",
    "\n",
    "問題73で学習したモデルの開発セットにおける正解率を求めよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1f1ef20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7649\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate(model, dataset, device):\n",
    "    model.eval() \n",
    "\n",
    "    y_preds = []  \n",
    "    y_trues = [] \n",
    "\n",
    "    for data in dataset:\n",
    "        input_ids = data[\"input_ids\"].to(device)\n",
    "        labels = data[\"label\"].to(device)\n",
    "\n",
    "        with torch.no_grad():  # 勾配計算を無効化（高速化・メモリ削減）\n",
    "            outputs = model(input_ids)  \n",
    "            probs = (\n",
    "                torch.sigmoid(outputs).squeeze().cpu().numpy()\n",
    "            )  # 確率に変換\n",
    "\n",
    "            y_preds.append(probs)\n",
    "            y_trues.append(labels.item())  # .item() で整数に変換\n",
    "\n",
    "    y_preds = np.array(y_preds)\n",
    "    y_trues = np.array(y_trues)\n",
    "\n",
    "    binary_preds = (y_preds > 0.5).astype(int)\n",
    "    accuracy = accuracy_score(y_trues, binary_preds)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "accuracy = evaluate(model, dev_dataset, device)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O08V9g0mcJwe",
   "metadata": {
    "id": "O08V9g0mcJwe"
   },
   "source": [
    "## 75. パディング\n",
    "\n",
    "複数の事例が与えられたとき、これらをまとめて一つのテンソル・オブジェクトで表現する関数`collate`を実装せよ。与えられた複数の事例のトークン列の長さが異なるときは、トークン列の長さが最も長いものに揃え、0番のトークンIDでパディングをせよ。さらに、トークン列の長さが長いものから順に、事例を並び替えよ。\n",
    "\n",
    "例えば、訓練データセットの冒頭の4事例が次のように表されているとき、\n",
    "\n",
    "```\n",
    "[{'text': 'hide new secretions from the parental units',\n",
    "  'label': tensor([0.]),\n",
    "  'input_ids': tensor([  5785,     66, 113845,     18,     12,  15095,   1594])},\n",
    " {'text': 'contains no wit , only labored gags',\n",
    "  'label': tensor([0.]),\n",
    "  'input_ids': tensor([ 3475,    87, 15888,    90, 27695, 42637])},\n",
    " {'text': 'that loves its characters and communicates something rather beautiful about human nature',\n",
    "  'label': tensor([1.]),\n",
    "  'input_ids': tensor([    4,  5053,    45,  3305, 31647,   348,   904,  2815,    47,  1276,  1964])},\n",
    " {'text': 'remains utterly satisfied to remain the same throughout',\n",
    "  'label': tensor([0.]),\n",
    "  'input_ids': tensor([  987, 14528,  4941,   873,    12,   208,   898])}]\n",
    "```\n",
    "\n",
    "`collate`関数を通した結果は以下のようになることが想定される。\n",
    "\n",
    "```\n",
    "{'input_ids': tensor([\n",
    "    [     4,   5053,     45,   3305,  31647,    348,    904,   2815,     47,   1276,   1964],\n",
    "    [  5785,     66, 113845,     18,     12,  15095,   1594,      0,      0,      0,      0],\n",
    "    [   987,  14528,   4941,    873,     12,    208,    898,      0,      0,      0,      0],\n",
    "    [  3475,     87,  15888,     90,  27695,  42637,      0,      0,      0,      0,      0]]),\n",
    " 'label': tensor([\n",
    "    [1.],\n",
    "    [0.],\n",
    "    [0.],\n",
    "    [0.]])}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9619064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     4,   5053,     45,   3305,  31647,    348,    904,   2815,     47,\n",
      "           1276,   1964],\n",
      "        [     6,     12,   1445,  43789,     12,  10946,     76,  41349,     42,\n",
      "              0,      0],\n",
      "        [  5785,     66, 113845,     18,     12,  15095,   1594,      0,      0,\n",
      "              0,      0],\n",
      "        [   987,  14528,   4941,    873,     12,    208,    898,      0,      0,\n",
      "              0,      0],\n",
      "        [  3475,     87,  15888,     90,  27695,  42637,      0,      0,      0,\n",
      "              0,      0]]),\n",
      " 'label': tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import torch\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "def collate(dataset):\n",
    "    # トークン列の長さ（＝\"input_ids\"の要素数）でソート\n",
    "    dataset = sorted(dataset, key=lambda x: len(x[\"input_ids\"]), reverse=True)\n",
    "    \n",
    "    input_ids = [data[\"input_ids\"] for data in dataset]\n",
    "    labels = [data[\"label\"] for data in dataset]\n",
    "    \n",
    "    # 最長のトークン列に合わせてパディング\n",
    "    input_ids = rnn_utils.pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    # ラベルはそのままスタック\n",
    "    labels = torch.stack(labels)\n",
    "    \n",
    "    return {\"input_ids\": input_ids, \"label\": labels}\n",
    "\n",
    "pprint(collate(train_dataset[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9NzvuZ-5ebDU",
   "metadata": {
    "id": "9NzvuZ-5ebDU"
   },
   "source": [
    "## 76. ミニバッチ学習\n",
    "\n",
    "問題75のパディングの処理を活用して、ミニバッチでモデルを学習せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc7c79e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  Training Loss: 0.4942\n",
      "  Training Accuracy: 0.8247\n",
      "  Validation Accuracy: 0.7718\n",
      "Epoch 2/5\n",
      "  Training Loss: 0.4063\n",
      "  Training Accuracy: 0.8336\n",
      "  Validation Accuracy: 0.7798\n",
      "Epoch 3/5\n",
      "  Training Loss: 0.3881\n",
      "  Training Accuracy: 0.8382\n",
      "  Validation Accuracy: 0.7867\n",
      "Epoch 4/5\n",
      "  Training Loss: 0.3805\n",
      "  Training Accuracy: 0.8399\n",
      "  Validation Accuracy: 0.7913\n",
      "Epoch 5/5\n",
      "  Training Loss: 0.3763\n",
      "  Training Accuracy: 0.8415\n",
      "  Validation Accuracy: 0.7993\n",
      "Final Validation Accuracy: 0.7993\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "class SSTDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "# モデルの定義を修正してバッチ処理に対応\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, embedding_matrix, freeze_embedding=True):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix, dtype=torch.float32),\n",
    "            freeze=freeze_embedding,\n",
    "        )\n",
    "        self.linear = nn.Linear(embedding_matrix.shape[1], 1)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        # [batch_size, seq_len] -> [batch_size, seq_len, emb_dim]\n",
    "        embedded = self.embedding(input_ids)\n",
    "        # 各シーケンスの平均ベクトルを計算 [batch_size, emb_dim]\n",
    "        # パディングを無視するためのマスク作成\n",
    "        mask = (input_ids != 0).float().unsqueeze(-1)  # [batch_size, seq_len, 1]\n",
    "        sum_embeddings = torch.sum(embedded * mask, dim=1)  # [batch_size, emb_dim]\n",
    "        sum_mask = torch.sum(mask, dim=1)  # [batch_size, 1]\n",
    "        mean_embeddings = sum_embeddings / sum_mask  # [batch_size, emb_dim]\n",
    "        \n",
    "        return self.linear(mean_embeddings)  # [batch_size, 1]\n",
    "\n",
    "# データローダーの作成\n",
    "batch_size = 32\n",
    "train_dataset_obj = SSTDataset(train_dataset)\n",
    "dev_dataset_obj = SSTDataset(dev_dataset)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset_obj, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate\n",
    ")\n",
    "\n",
    "dev_loader = DataLoader(\n",
    "    dev_dataset_obj, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate\n",
    ")\n",
    "\n",
    "# モデルの初期化\n",
    "model = LogisticRegression(embedding_matrix, freeze_embedding=True).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 学習\n",
    "def train_batch(model, data_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * input_ids.size(0)\n",
    "    \n",
    "    return total_loss / len(data_loader.dataset)\n",
    "\n",
    "# 評価\n",
    "def evaluate_batch(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids)\n",
    "            predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            \n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    return correct / total\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_batch(model, train_loader, optimizer, criterion, device)\n",
    "    train_acc = evaluate_batch(model, train_loader, device)\n",
    "    dev_acc = evaluate_batch(model, dev_loader, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"  Training Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Training Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"  Validation Accuracy: {dev_acc:.4f}\")\n",
    "\n",
    "# 最終的な評価\n",
    "final_accuracy = evaluate_batch(model, dev_loader, device)\n",
    "print(f\"Final Validation Accuracy: {final_accuracy:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"./model/No_76.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RUbjivUTejxn",
   "metadata": {
    "id": "RUbjivUTejxn"
   },
   "source": [
    "## 77. GPU上での学習\n",
    "\n",
    "問題76のモデル学習をGPU上で実行せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b7e8777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  Training Loss: 0.4944\n",
      "  Training Accuracy: 0.8277\n",
      "  Validation Accuracy: 0.7810\n",
      "Epoch 2/5\n",
      "  Training Loss: 0.4064\n",
      "  Training Accuracy: 0.8344\n",
      "  Validation Accuracy: 0.7890\n",
      "Epoch 3/5\n",
      "  Training Loss: 0.3882\n",
      "  Training Accuracy: 0.8374\n",
      "  Validation Accuracy: 0.7821\n",
      "Epoch 4/5\n",
      "  Training Loss: 0.3804\n",
      "  Training Accuracy: 0.8371\n",
      "  Validation Accuracy: 0.7821\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 102\u001b[0m\n\u001b[0;32m    100\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m--> 102\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m evaluate_batch(model, train_loader, device)\n\u001b[0;32m    104\u001b[0m     dev_acc \u001b[38;5;241m=\u001b[39m evaluate_batch(model, dev_loader, device)\n",
      "Cell \u001b[1;32mIn[9], line 64\u001b[0m, in \u001b[0;36mtrain_batch\u001b[1;34m(model, data_loader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     61\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     62\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 64\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 13\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m     10\u001b[0m labels \u001b[38;5;241m=\u001b[39m [data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataset]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 最長のトークン列に合わせてパディング\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mrnn_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ラベルはそのままスタック\u001b[39;00m\n\u001b[0;32m     15\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(labels)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\utils\\rnn.py:478\u001b[0m, in \u001b[0;36mpad_sequence\u001b[1;34m(sequences, batch_first, padding_value, padding_side)\u001b[0m\n\u001b[0;32m    474\u001b[0m         sequences \u001b[38;5;241m=\u001b[39m sequences\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# assuming trailing dimensions and type of all the Tensors\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;66;03m# in sequences are same and fetching those from sequences[0]\u001b[39;00m\n\u001b[1;32m--> 478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# devide = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SSTDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "\n",
    "# モデルの定義を修正してバッチ処理に対応\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, embedding_matrix, freeze_embedding=True):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix, dtype=torch.float32),\n",
    "            freeze=freeze_embedding,\n",
    "        )\n",
    "        self.linear = nn.Linear(embedding_matrix.shape[1], 1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # [batch_size, seq_len] -> [batch_size, seq_len, emb_dim]\n",
    "        embedded = self.embedding(input_ids)\n",
    "        # 各シーケンスの平均ベクトルを計算 [batch_size, emb_dim]\n",
    "        # パディングを無視するためのマスク作成\n",
    "        mask = (input_ids != 0).float().unsqueeze(-1)  # [batch_size, seq_len, 1]\n",
    "        sum_embeddings = torch.sum(embedded * mask, dim=1)  # [batch_size, emb_dim]\n",
    "        sum_mask = torch.sum(mask, dim=1)  # [batch_size, 1]\n",
    "        mean_embeddings = sum_embeddings / sum_mask  # [batch_size, emb_dim]\n",
    "\n",
    "        return self.linear(mean_embeddings)  # [batch_size, 1]\n",
    "\n",
    "\n",
    "# データローダーの作成\n",
    "batch_size = 32\n",
    "train_dataset_obj = SSTDataset(train_dataset)\n",
    "dev_dataset_obj = SSTDataset(dev_dataset)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset_obj, batch_size=batch_size, shuffle=True, collate_fn=collate\n",
    ")\n",
    "\n",
    "dev_loader = DataLoader(\n",
    "    dev_dataset_obj, batch_size=batch_size, shuffle=False, collate_fn=collate\n",
    ")\n",
    "\n",
    "# モデルの初期化\n",
    "model = LogisticRegression(embedding_matrix, freeze_embedding=True).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "# 学習\n",
    "def train_batch(model, data_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "    return total_loss / len(data_loader.dataset)\n",
    "\n",
    "\n",
    "# 評価\n",
    "def evaluate_batch(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_batch(model, train_loader, optimizer, criterion, device)\n",
    "    train_acc = evaluate_batch(model, train_loader, device)\n",
    "    dev_acc = evaluate_batch(model, dev_loader, device)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"  Training Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Training Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"  Validation Accuracy: {dev_acc:.4f}\")\n",
    "\n",
    "# 最終的な評価\n",
    "final_accuracy = evaluate_batch(model, dev_loader, device)\n",
    "print(f\"Final Validation Accuracy: {final_accuracy:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"./model/No_77.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZUY1PsD-eplq",
   "metadata": {
    "id": "ZUY1PsD-eplq"
   },
   "source": [
    "## 78. 単語埋め込みのファインチューニング\n",
    "\n",
    "問題77の学習において、単語埋め込みのパラメータも同時に更新するファインチューニングを導入せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d8c3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  Training Loss: 0.4953\n",
      "  Training Accuracy: 0.8270\n",
      "  Validation Accuracy: 0.7798\n",
      "Epoch 2/5\n",
      "  Training Loss: 0.4066\n",
      "  Training Accuracy: 0.8340\n",
      "  Validation Accuracy: 0.7913\n",
      "Epoch 3/5\n",
      "  Training Loss: 0.3883\n",
      "  Training Accuracy: 0.8383\n",
      "  Validation Accuracy: 0.7890\n",
      "Epoch 4/5\n",
      "  Training Loss: 0.3806\n",
      "  Training Accuracy: 0.8404\n",
      "  Validation Accuracy: 0.7970\n",
      "Epoch 5/5\n",
      "  Training Loss: 0.3764\n",
      "  Training Accuracy: 0.8407\n",
      "  Validation Accuracy: 0.7936\n",
      "Final Validation Accuracy: 0.7936\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# devide = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class SSTDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    # freeze_embedding を False にすると埋め込み層の重みを学習する\n",
    "    def __init__(self, embedding_matrix, freeze_embedding=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix, dtype=torch.float32),\n",
    "            freeze=freeze_embedding,\n",
    "        )\n",
    "        self.linear = nn.Linear(embedding_matrix.shape[1], 1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # [batch_size, seq_len] -> [batch_size, seq_len, emb_dim]\n",
    "        embedded = self.embedding(input_ids)\n",
    "        # 各シーケンスの平均ベクトルを計算 [batch_size, emb_dim]\n",
    "        # パディングを無視するためのマスク作成\n",
    "        mask = (input_ids != 0).float().unsqueeze(-1)  # [batch_size, seq_len, 1]\n",
    "        sum_embeddings = torch.sum(embedded * mask, dim=1)  # [batch_size, emb_dim]\n",
    "        sum_mask = torch.sum(mask, dim=1)  # [batch_size, 1]\n",
    "        mean_embeddings = sum_embeddings / sum_mask  # [batch_size, emb_dim]\n",
    "\n",
    "        return self.linear(mean_embeddings)  # [batch_size, 1]\n",
    "\n",
    "\n",
    "# データローダーの作成\n",
    "batch_size = 32\n",
    "train_dataset_obj = SSTDataset(train_dataset)\n",
    "dev_dataset_obj = SSTDataset(dev_dataset)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset_obj, batch_size=batch_size, shuffle=True, collate_fn=collate\n",
    ")\n",
    "\n",
    "dev_loader = DataLoader(\n",
    "    dev_dataset_obj, batch_size=batch_size, shuffle=False, collate_fn=collate\n",
    ")\n",
    "\n",
    "# モデルの初期化\n",
    "model = LogisticRegression(embedding_matrix, freeze_embedding=True).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "# 学習\n",
    "def train_batch(model, data_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "    return total_loss / len(data_loader.dataset)\n",
    "\n",
    "\n",
    "# 評価\n",
    "def evaluate_batch(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_batch(model, train_loader, optimizer, criterion, device)\n",
    "    train_acc = evaluate_batch(model, train_loader, device)\n",
    "    dev_acc = evaluate_batch(model, dev_loader, device)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"  Training Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Training Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"  Validation Accuracy: {dev_acc:.4f}\")\n",
    "\n",
    "# 最終的な評価\n",
    "final_accuracy = evaluate_batch(model, dev_loader, device)\n",
    "print(f\"Final Validation Accuracy: {final_accuracy:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"./model/No_77.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jVAdWIq0evKR",
   "metadata": {
    "id": "jVAdWIq0evKR"
   },
   "source": [
    "## 79. アーキテクチャの変更\n",
    "\n",
    "ニューラルネットワークのアーキテクチャを自由に変更し、モデルを学習せよ。また、学習したモデルの開発セットにおける正解率を求めよ。例えば、テキストの特徴ベクトル（単語埋め込みの平均ベクトル）に対して多層のニューラルネットワークを通したり、畳み込みニューラルネットワーク（CNN; Convolutional Neural Network）や再帰型ニューラルネットワーク（RNN; Recurrent Neural Network）などのモデルの学習に挑戦するとよい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d5bfbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  Training Loss: 0.6799\n",
      "  Training Accuracy: 0.5492\n",
      "  Validation Accuracy: 0.5115\n",
      "Epoch 2/5\n",
      "  Training Loss: 0.6874\n",
      "  Training Accuracy: 0.5569\n",
      "  Validation Accuracy: 0.5229\n",
      "Epoch 3/5\n",
      "  Training Loss: 0.6877\n",
      "  Training Accuracy: 0.5582\n",
      "  Validation Accuracy: 0.5092\n",
      "Epoch 4/5\n",
      "  Training Loss: 0.6884\n",
      "  Training Accuracy: 0.5582\n",
      "  Validation Accuracy: 0.5092\n",
      "Epoch 5/5\n",
      "  Training Loss: 0.6891\n",
      "  Training Accuracy: 0.5582\n",
      "  Validation Accuracy: 0.5092\n",
      "Final Validation Accuracy: 0.5092\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class SSTDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, freeze_embedding=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(embedding_matrix, dtype=torch.float32),\n",
    "            freeze=freeze_embedding,\n",
    "        )\n",
    "        emb_dim = embedding_matrix.shape[1]\n",
    "        self.l1 = nn.RNN(emb_dim, hidden_dim, nonlinearity=\"tanh\", batch_first=True)\n",
    "        self.l2 = nn.Linear(hidden_dim, 1)\n",
    "        nn.init.xavier_normal_(self.l1.weight_ih_l0)\n",
    "        nn.init.orthogonal_(self.l1.weight_hh_l0)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # [batch_size, seq_len] -> [batch_size, seq_len, emb_dim]\n",
    "        embedded = self.embedding(input_ids)\n",
    "        \n",
    "        # RNNの処理\n",
    "        h, _ = self.l1(embedded)\n",
    "        # 最後のタイムステップの隠れ状態を使用\n",
    "        y = self.l2(h[:, -1])\n",
    "        return y\n",
    "\n",
    "\n",
    "# データローダーの作成\n",
    "batch_size = 32\n",
    "train_dataset_obj = SSTDataset(train_dataset)\n",
    "dev_dataset_obj = SSTDataset(dev_dataset)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset_obj, batch_size=batch_size, shuffle=True, collate_fn=collate\n",
    ")\n",
    "\n",
    "dev_loader = DataLoader(\n",
    "    dev_dataset_obj, batch_size=batch_size, shuffle=False, collate_fn=collate\n",
    ")\n",
    "\n",
    "# モデルの初期化\n",
    "model = RNN(embedding_matrix, hidden_dim=128, freeze_embedding=True).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "# 学習\n",
    "def train_batch(model, data_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "    return total_loss / len(data_loader.dataset)\n",
    "\n",
    "\n",
    "# 評価\n",
    "def evaluate_batch(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_batch(model, train_loader, optimizer, criterion, device)\n",
    "    train_acc = evaluate_batch(model, train_loader, device)\n",
    "    dev_acc = evaluate_batch(model, dev_loader, device)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"  Training Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Training Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"  Validation Accuracy: {dev_acc:.4f}\")\n",
    "\n",
    "# 最終的な評価\n",
    "final_accuracy = evaluate_batch(model, dev_loader, device)\n",
    "print(f\"Final Validation Accuracy: {final_accuracy:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"./model/No_79.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
